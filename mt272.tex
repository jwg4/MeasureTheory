\frfilename{mt272.tex}
\versiondate{3.4.09}
\copyrightdate{2000}

\def\hatcalBnu{\hat\Cal B_{\nu}}

\def\chaptername{Probability theory}
\def\sectionname{Independence}

\newsection{272}

I introduce the concept of `independence' for families of events,
$\sigma$-algebras and random variables.   The first part of the section,
down to 272G, amounts to an analysis of the elementary relationships
between the three manifestations of the idea.   In 272G I give the
fundamental result that the joint distribution of a (finite) independent
family of random variables is just the product of the individual
distributions.   Further expressions of the connexion between
independence and product measures are in 272J, 272M and 272N.
I give a
version of the zero-one law (272O), and I end the section with a
group of basic results from probability theory concerning sums and
products of independent random variables (272R-272W).
%272R 272S 272T 272U 272V 272W

\vleader{48pt}{272A}{Definitions} Let $(\Omega,\Sigma,\mu)$ be a 
probability space.

\header{272Aa}{\bf (a)} A family $\langle E_i\rangle_{i\in I}$ in
$\Sigma$ is {\bf (stochastically) independent} if

\Centerline{$\mu(E_{i_1}\cap E_{i_2}\cap\ldots\cap E_{i_n})
=\prod_{j=1}^n\mu E_{i_j}$}

\noindent whenever $i_1,\ldots,i_n$ are distinct members of $I$.

\header{272Ab}{\bf (b)} A family $\langle\Sigma_i\rangle_{i\in I}$ of
$\sigma$-subalgebras of $\Sigma$ is {\bf (stochastically) independent}
if

\Centerline{$\mu(E_{1}\cap E_{2}\cap\ldots\cap E_{n})
=\prod_{j=1}^n\mu E_{j}$}

\noindent whenever $i_1,\ldots,i_n$ are distinct members of $I$ and
$E_j\in\Sigma_{i_j}$ for every $j\le n$.

\header{272Ac}{\bf (c)} A family $\langle X_i\rangle_{i\in I}$ of
real-valued random variables on $\Omega$ is {\bf (stochastically)
independent} if

\Centerline{$\Pr(X_{i_j}\le\alpha_j\text{ for every }j\le n)
=\prod_{j=1}^n\Pr(X_{i_j}\le \alpha_j)$}

\noindent whenever $i_1,\ldots,i_n$ are distinct members of $I$ and
$\alpha_1,\ldots,\alpha_n\in\Bbb R$.

\cmmnt{
\leader{272B}{Remarks (a)} This is perhaps the central
contribution of
probability theory to measure theory, and as such deserves the most
careful scrutiny.   The idea of `independence' comes from outside
mathematics altogether, in the notion of events which have independent
causes.  I suppose that 272G and 272M are the results below which most
clearly show the measure-theoretic aspects of the concept.   It is not
an accident that both involve product measures;  one of the wonders of
measure theory is the fact that the same technical devices are used in
establishing the probability theory of stochastic independence and the
geometry of multi-dimensional volume.

\header{272Bb}
{\bf (b)} In the following paragraphs I will try to describe some
relationships between the three notions of independence just defined.
But it is worth noting at once the fact that, in all three cases, a
family is independent iff all its finite subfamilies are independent.
Consequently any subfamily of an independent family is independent.
Another elementary fact which is immediate from the definitions is that
if $\langle \Sigma_i\rangle_{i\in I}$ is an independent family of
$\sigma$-algebras, and $\Sigma'_i$ is a $\sigma$-subalgebra of
$\Sigma_i$ for each $i$, then $\langle\Sigma'_i\rangle_{i\in I}$ is an
independent family.

\header{272Bc}
{\bf (c)} A useful reformulation of 272Ab is the following:  A family
$\langle\Sigma_i\rangle_{i\in I}$ of $\sigma$-subalgebras of $\Sigma$ is
independent iff

\Centerline{$\mu(\bigcap_{i\in I}E_i)=\prod_{i\in I}\mu E_i$}

\noindent whenever $E_i\in\Sigma_i$ for every $i$ and
$\{i:E_i\ne\Omega\}$ is finite.   (Here I follow the convention of 254F,
saying that for a family $\langle\alpha_i\rangle_{i\in I}$ in $[0,1]$ we
take $\prod_{i\in I}\alpha_i=1$ if $I=\emptyset$, and otherwise it is to
be $\inf_{J\subseteq I,J\text{ is finite}}\prod_{i\in J}\alpha_j$.)

\header{272Bd}
{\bf (d)} In 272Aa-b I speak of sets $E_i\in\Sigma$ and algebras
$\Sigma_i\subseteq\Sigma$.   In fact (272Ac already gives a hint of
this) we shall more often than not be concerned with $\hat\Sigma$
rather than with $\Sigma$, if there is a difference, where
$(\Omega,\hat\Sigma,\hat\mu)$ is the completion of
$(\Omega,\Sigma,\mu)$.
}%end of comment

\leader{272C}{The $\sigma$-subalgebra defined by a random
variable}\cmmnt{ To relate
272Ab to 272Ac we need the following notion.}   Let
$(\Omega,\Sigma,\mu)$
be a probability space and $X$ a real-valued random variable defined on
$\Omega$.   Write $\Cal B$ for the
$\sigma$-algebra of Borel subsets of $\Bbb R$, and $\Sigma_X$ for

\Centerline{$\{X^{-1}[F]:F\in\Cal B\}
\cup\{(\Omega\setminus\dom X)\cup X^{-1}[F]:F\in\Cal B\}$.}

\noindent Then $\Sigma_X$ is a $\sigma$-algebra of subsets of $\Omega$.
\prooflet{\Prf\

\Centerline{$\emptyset=X^{-1}[\emptyset]\in\Sigma_X$;}

\noindent if $F\in\Cal B$ then

\Centerline{$\Omega\setminus X^{-1}[F]
=(\Omega\setminus\dom X)\cup X^{-1}[\Bbb R\setminus F]\in\Sigma_X$,}

\Centerline{$\Omega\setminus((\Omega\setminus\dom X)\cup X^{-1}[F])
=X^{-1}[\Bbb R\setminus F]\in\Sigma_X$;}

\noindent if $\sequence{k}{F_k}$ is any sequence in $\Cal B$ then

\Centerline{$\bigcup_{k\in\Bbb N}X^{-1}[F_k]=X^{-1}[\bigcup_{k\in\Bbb
N}F_k]$,}

\noindent so

\Centerline{$\bigcup_{k\in\Bbb N}X^{-1}[F_k]$,
\quad $(\Omega\setminus\dom X)\cup\bigcup_{k\in\Bbb N}X^{-1}[F_k]$}

\noindent belong to $\Sigma_X$.   \Qed}

\cmmnt{Evidently }$\Sigma_X$ is the smallest $\sigma$-algebra of
subsets of $\Omega$, containing $\dom X$, for which $X$ is measurable.
\cmmnt{Also }$\Sigma_X$ is a subalgebra of
$\hat\Sigma$, where $\hat\Sigma$ is the domain of the completion of
$\mu$\cmmnt{ (271Aa)}.

\cmmnt{Now we have the following result.}

\leader{272D}{Proposition} Let $(\Omega,\Sigma,\mu)$ be a probability
space and $\langle X_i\rangle_{i\in I}$ a family of real-valued random
variables on $\Omega$.   For each $i\in I$, let $\Sigma_i$ be the
$\sigma$-algebra defined by $X_i$\cmmnt{, as in 272C}.   Then the
following are equiveridical:

(i) $\langle X_i\rangle_{i\in I}$ is independent;

(ii) whenever $i_1,\ldots,i_n$ are distinct members of $I$ and
$F_1,\ldots,F_n$ are Borel subsets of $\Bbb R$, then

\Centerline{$\Pr(X_{i_j}\in F_j\text{ for every }j\le n)
=\prod_{j=1}^n\Pr(X_{i_j}\in F_j)$;}

(iii) whenever $\langle F_i\rangle_{i\in I}$ is a family of Borel
subsets of $\Bbb R$, and $\{i:F_i\ne\Bbb R\}$ is finite, then

\Centerline{$\hat\mu\bigl(
\bigcap_{i\in I}(X_i^{-1}[F_i]\cup(\Omega\setminus\dom X_i))\bigr)
=\prod_{i\in I}\Pr(X_i\in F_i)$,}

\noindent where $\hat\mu$ is the completion of $\mu$;

(iv) $\langle\Sigma_i\rangle_{i\in I}$ is independent with respect to
$\hat\mu$.

\proof{{\bf (a)(i)$\Rightarrow$(ii)} Write
$\pmb{X}=(X_{i_1},\ldots,X_{i_n})$.   Write $\nu_{\pmb{X}}$ for the
joint distribution of $\pmb{X}$, and for each $j\le n$ write $\nu_j$ for
the distribution of $X_{i_j}$;  let  $\nu$ be the product of
$\nu_1,\ldots,\nu_n$ as described in 254A-254C.   (I
wrote \S254 out as for infinite products.   If you are interested
only in finite products of probability spaces, which are adequate for
our needs in this paragraph, I recommend reading \S\S251-252 with the
mental proviso that all measures are probabilities, and then \S254
with the proviso that the set $I$ is finite.)   By 256K, $\nu$ is a
Radon measure on $\BbbR^n$.   (This is an induction on $n$, relying on
254N for assurance that we can regard $\nu$ as the repeated product
$(\ldots((\nu_1\times\nu_2)\times\nu_3)\times\ldots\nu_{n-1})
\times\nu_n$.)   Then for any
$a=(\alpha_1,\ldots,\alpha_n)\in\BbbR^n$, we have

$$\eqalignno{\nu\ocint{-\infty,a}
&=\nu\bigl(\prod_{j=1}^n\ocint{-\infty,\alpha_j}\bigr)
=\prod_{j=1}^n\nu_{j}\ocint{-\infty,\alpha_j}\cr
\noalign{\noindent (using 254Fb)}
&=\prod_{j=1}^n\Pr(X_{i_j}\le\alpha_j)
=\Pr(X_{i_j}\le\alpha_j\text{ for every }j\le n)\cr
\noalign{\noindent (using the condition (i))}
&=\nu_{\pmb{X}}\ocint{-\infty,a}.\cr}$$

\noindent By the uniqueness assertion in 271Ba, $\nu=\nu_{\pmb{X}}$.
In particular, if $F_1,\ldots,F_n$ are Borel subsets of $\Bbb R$,

$$\eqalign{\Pr(X_{i_j}\in F_j\text{ for every }j\le n)
&=\Pr(\pmb{X}\in\prod_{j\le n}F_j)
=\nu_{\pmb{X}}(\prod_{j\le n}F_j)\cr
&=\nu(\prod_{j\le n}F_j)
=\prod_{j=1}^n\nu_{{j}}F_j
=\prod_{j=1}^n\Pr(X_{i_j}\in F_j),\cr}$$

\noindent as required.


\medskip

{\bf (b)(ii)$\Rightarrow$(i)} is trivial, if we recall that all sets
$\ocint{-\infty,\alpha}$ are Borel sets, so that the definition of
independence given in 272Ac is just a special case of (ii).

\medskip

{\bf (c)(ii)$\Rightarrow$(iv)} Assume (ii), and suppose that
$i_1,\ldots,i_n$ are distinct members of $I$ and $E_j\in\Sigma_{{i_j}}$
for each $j\le n$.    For each $j$, set $E'_j=E_j\cap\dom X_{i_j}$, so
that $E'_j$ may be expressed as $X_{i_j}^{-1}[F_j]$ for some Borel set
$F_j\subseteq\Bbb R$.   Then $\hat\mu(E_j\setminus E'_j)=0$ for each
$j$, so

$$\eqalignno{\hat\mu(\bigcap_{1\le j\le n}E_j)
&=\hat\mu(\bigcap_{1\le j\le n}E'_j)
=\Pr(X_{i_1}\in F_1,\ldots,X_{i_n}\in F_n)\cr
&=\prod_{j=1}^n\Pr(X_{i_j}\in F_j)\cr
\noalign{\noindent (using (ii))}
&=\prod_{i=1}^n\hat\mu E_j.\cr}$$

\noindent As $E_1,\ldots,E_k$ are arbitrary,
$\langle\Sigma_{i}\rangle_{i\in I}$ is independent.

\medskip

{\bf (d)(iv)$\Rightarrow$(ii)}
Now suppose that $\langle\Sigma_{i}\rangle_{i\in I}$ is independent.
If $i_1,\ldots,i_n$ are distinct members of $I$ and $F_1,\ldots,F_n$ are
Borel sets in $\Bbb R$, then $X_{i_j}^{-1}[F_j]\in\Sigma_{i_j}$ for each
$j$, so

$$\eqalignno{\Pr(X_{i_1}\in F_1,\ldots,X_{i_n}\in F_n)
&=\hat\mu(\bigcap_{1\le j\le n}X_{i_j}^{-1}[F_j])\cr
&=\prod_{i=1}^n\hat\mu X_{i_j}^{-1}[F_j]
=\prod_{j=1}^n\Pr(X_{i_j}\in F_j)\cr.}$$

\medskip

%272D
{\bf (e)} Finally, observe that (iii) is nothing but a re-formulation of
(ii), because if $F_i=\Bbb R$ then $\Pr(X_i\in F_i)=1$ and
$X_i^{-1}[F_i]\cup(\Omega\setminus\dom X_i)=\Omega$.
}%end of proof of 272D

\leader{272E}{Corollary} Let $\langle X_i\rangle_{i\in I}$ be an
independent family of real-valued random variables, and
$\langle h_i\rangle_{i\in I}$ any family of Borel measurable functions
from $\Bbb R$ to $\Bbb R$.   Then $\langle h_i(X_i)\rangle_{i\in I}$ is
independent.

\proof{ Writing $\Sigma_i$ for the $\sigma$-algebra defined by $X_i$,
$\Sigma'_i$ for the $\sigma$-algebra defined by $h(X_i)$, $h(X_i)$ is
$\Sigma_i$-measurable (121Eg) so $\Sigma'_i\subseteq\Sigma_i$ for every
$i$ and $\langle\Sigma'_i\rangle_{i\in I}$ is independent, as in 272Bb.
}%end of proof of 272E

\leader{272F}{}\cmmnt{ Similarly, we can relate the definition in
272Aa to the others.

\medskip

\noindent}{\bf Proposition} Let $(\Omega,\Sigma,\mu)$ be a probability
space, and $\langle E_i\rangle_{i\in I}$ a family in $\Sigma$.   Set
$\Sigma_i=\{\emptyset,E_i,\Omega\setminus E_i,\Omega\}$, the
($\sigma$-\nobreak)algebra of subsets of $\Omega$ generated by $E_i$, and
$X_i=\chi E_i$, the indicator function of $E_i$.   Then the
following are equiveridical:

(i) $\langle E_i\rangle_{i\in I}$ is independent;

(ii) $\langle \Sigma_i\rangle_{i\in I}$ is independent;

(iii) $\langle X_i\rangle_{i\in I}$ is independent.

\proof{{\bf (i)$\Rightarrow$(iii)} If $i_1,\ldots,i_n$ are distinct
members of $I$ and $\alpha_1,\ldots,\alpha_n\in\Bbb R$, then for each
$j\le n$ the set $G_j=\{\omega:X_{i_j}(\omega)\le\alpha_j\}$ is either
$E_{i_j}$ or $\emptyset$ or $\Omega$.   If any $G_j$ is empty, then

\Centerline{$\Pr(X_{i_j}\le\alpha_j\text{ for every}j\le
n\}=0=\prod_{j=1}^n\Pr(X_{i_j}\le\alpha_j)$.}

\noindent Otherwise, set $K=\{j:G_j=E_{i_j}\}$;  then

$$\eqalign{\Pr(X_{i_j}\le\alpha_j\text{ for every}j\le n\}
&=\mu(\bigcap_{j\le n}G_j)
=\mu(\bigcap_{j\in K}E_{i_j})\cr
&=\prod_{j\in K}\mu E_{i_j}
=\prod_{j=1}^n\Pr(X_{i_j}\le\alpha_j).\cr}$$

\noindent As $i_1,\ldots,i_n$ and $\alpha_1,\ldots,\alpha_n$ are
arbitrary, $\langle X_i\rangle_{i\in I}$ is independent.

\medskip

{\bf (iii)$\Rightarrow$(ii)} follows from (i)$\Rightarrow$(iii) of 272D,
because $\Sigma_i$ is the $\sigma$-algebra defined by $X_i$.

\medskip

{\bf (ii)$\Rightarrow$(i)} is trivial, because $E_i\in\Sigma_i$ for each
$i$.
}%end of proof of 272F

\cmmnt{\medskip

\noindent{\bf Remark} You will I hope feel that while the theory of
product measures might be appropriate to 272D, it is surely rather heavy
machinery to use on what ought to be a simple combinatorial problem like
(iii)$\Rightarrow$(ii) of this proposition.   I suggest that you
construct an `elementary' proof, and examine which of the ideas of
the theory of product measures (and the Monotone Class Theorem, 136B)
are actually needed here.
}%end of comment

\leader{272G}{Distributions of independent
random \dvrocolon{variables}}\cmmnt{ I have
not tried to describe the `joint distribution' of an infinite family of
random variables.   (Indications of how to deal with a countable family
are offered in 271Ya and 272Yb.   For uncountable families I will wait
until \S454 in Volume 4.)   As, however, the independence of a
family of random variables is determined by the behaviour of finite
subfamilies, we can approach it through the following proposition.

\medskip

\noindent}{\bf Theorem} Let $\pmb{X}=(X_1,\ldots,X_n)$ be a finite
family of real-valued random variables on a probability space.   Let
$\nu_{\pmb{X}}$ be the corresponding distribution on $\BbbR^n$.   Then
the following are equiveridical:

(i) $X_1,\ldots,X_n$ are independent;

(ii) $\nu_{\pmb{X}}$ can be expressed as a product of $n$ probability
measures $\nu_1,\ldots,\nu_n$, one for each factor $\Bbb R$ of $\Bbb
R^n$;

(iii)  $\nu_{\pmb{X}}$ is the product measure of $\nu_{X_1},\ldots,
\nu_{X_n}$, writing $\nu_{X_i}$ for the distribution of the random
variable $X_i$.

\proof{{\bf (a)(i)$\Rightarrow$(iii)} In the proof of
(i)$\Rightarrow$(ii) of 272D above I showed that
$\nu_{\pmb{X}}$ is the product $\nu$ of
$\nu_{X_1},\ldots,\nu_{X_n}$.

\medskip

{\bf (b)(iii)$\Rightarrow$(ii)} is trivial.

\medskip

{\bf (c)(ii)$\Rightarrow$(i)} Suppose that $\nu_{\pmb{X}}$ is
expressible as a product $\nu_1\times\ldots\times\nu_n$.   Take
$a=(\alpha_1,\ldots,\alpha_n)$ in $\BbbR^n$.  Then

\Centerline{$\Pr(X_i\le\alpha_i\Forall i\le n)
=\Pr(\pmb{X}\in\ocint{-\infty,a})
=\nu_{\pmb{X}}(\ocint{-\infty,a})
=\prod_{i=1}^n\nu_i\ocint{-\infty,\alpha_i}$.}

\noindent On the other hand, setting
$F_i=\{(\xi_1,\ldots,\xi_n):\xi_i\le\alpha_i\}$, we must have

\Centerline{$\nu_i\ocint{-\infty,\alpha_i}
=\nu_{\pmb{X}}F_i=\Pr(\pmb{X}\in F_i)
=\Pr(X_i\le\alpha_i)$}

\noindent for each $i$.   So we get

\Centerline{$\Pr(X_i\le\alpha_i\text{ for every }i\le n)
=\prod_{i=1}^n\Pr(X_i\le\alpha_i)$,}

\noindent as required.
}%end of proof of 272G

\leader{272H}{Corollary} Suppose that $\langle X_i\rangle_{i\in I}$ is
an independent family of real-valued random variables on a probability
space $(\Omega,\Sigma,\mu)$, and that for each $i\in I$ we are given
another real-valued random variable $Y_i$ on $\Omega$ such that
$Y_i\eae X_i$.   Then $\langle Y_i\rangle_{i\in I}$ is independent.

\proof{ For every distinct $i_1,\ldots,i_n\in I$, if we set
$\pmb{X}=(X_{i_1},\ldots,X_{i_n})$ and
$\pmb{Y}=(Y_{i_1},\ldots,Y_{i_n})$, then $\pmb{X}\eae\pmb{Y}$, so
$\nu_{\pmb{X}}$, $\nu_{\pmb{Y}}$ are equal (271De).   By 272G,
$Y_{i_1},\ldots,Y_{i_n}$ must be independent because
$X_{i_1},\ldots,X_{i_n}$ are.   As $i_1,\ldots,i_n$ are arbitrary, the
whole family $\langle Y_i\rangle_{i\in I}$ is independent.
}%end of proof of 272H

\cmmnt{\medskip

\noindent{\bf Remark} It follows that we may speak of independent
families in the space $L^0(\mu)$ of equivalence classes of
random variables (241C), saying that
$\langle X^{\ssbullet}_i\rangle_{i\in I}$ is independent iff
$\langle X_i\rangle_{i\in I}$ is.
}%end of comment

\leader{272I}{Corollary} Suppose that $X_1,\ldots,X_n$ are independent
real-valued random variables with density functions
$f_1,\ldots,f_n$\cmmnt{ (271H)}.
Then $\pmb{X}=(X_1,\ldots,X_n)$ has a density function $f$ given by
setting $f(x)=\prod_{i=1}^nf_i(\xi_i)$ whenever
$x=(\xi_1,\ldots,\xi_n)\in\prod_{i\le n}\dom(f_i)\subseteq\BbbR^n$.

\proof{ For $n=2$ this is covered by 253I;  the general case follows by
induction on $n$.
}%end of proof of 272I

\leader{272J}{}\cmmnt{ The most important theorems of the subject
refer to
independent families of random variables, rather than independent
families of $\sigma$-algebras.   The value of the concept of independent
$\sigma$-algebras lies in such results as the following.

\medskip

\noindent}{\bf Proposition} Let $(\Omega,\Sigma,\mu)$ be a complete
probability space, and $\langle\Sigma_i\rangle_{i\in I}$ a
family of $\sigma$-subalgebras of $\Sigma$.   For each $i\in I$ let
$\mu_i$ be the restriction of $\mu$ to $\Sigma_i$, and let
$(\Omega^I,\Lambda,\lambda)$ be the product probability space of the
family $\langle(\Omega,\Sigma_i,\mu_i)\rangle_{i\in I}$.   Define
$\phi:\Omega\to\Omega^I$ by setting $\phi(\omega)(i)=\omega$ whenever
$\omega\in\Omega$ and $i\in I$.   Then $\phi$ is \imp\ iff
$\langle\Sigma_i\rangle_{i\in I}$ is independent.

\proof{ This is virtually a restatement of 254Fb and 254G.   (i) If
$\phi$ is \imp, $i_1,\ldots,i_n\in I$ are distinct and
$E_j\in\Sigma_{i_j}$ for each $j$, then
$\bigcap_{j\le n}E_{i_j}=\phi^{-1}[\{x:x(i_j)\in E_j$ for every
$j\le n\}]$, so that

\Centerline{$\mu(\bigcap_{j\le n}E_{i_j})
=\lambda\{x:x(i_j)\in E_j\text{ for every }j\le n\}
=\prod_{j=1}^n\mu_{i_j}E_{i_j}
=\prod_{j=1}^n\mu E_{i_j}$.}

\noindent (ii) If $\langle\Sigma_i\rangle_{i\in I}$ is independent,
$E_i\in\Sigma_i$
for every $i\in I$ and $\{i:E_i\ne\Omega\}$ is finite, then

\Centerline{$\mu\phi^{-1}[\prod_{i\in I}E_i]
=\mu(\bigcap_{i\in I}E_i)
=\prod_{i\in I}\mu E_i
=\prod_{i\in I}\mu_iE_i$.}

\noindent So
the conditions of 254G are satisfied and $\mu\phi^{-1}[W]=\lambda W$ for
every $W\in\Lambda$.
}%end of proof of 272J

\leader{272K}{Proposition} Let $(\Omega,\Sigma,\mu)$ be a probability
space and $\langle\Sigma_i\rangle_{i\in I}$ an independent family of
$\sigma$-subalgebras of $\Sigma$.   Let
$\langle J(s)\rangle_{s\in S}$ be a disjoint family of subsets of $I$,
and for each $s\in S$ let
$\tilde\Sigma_s$ be the $\sigma$-algebra of subsets of $\Omega$
generated by $\bigcup_{i\in J(s)}\Sigma_i$.   Then
$\langle\tilde\Sigma_s\rangle_{s\in S}$ is independent.

\proof{ Let $(\Omega,\hat\Sigma,\hat\mu)$ be the completion of
$(\Omega,\Sigma,\mu)$.   On $\Omega^I$ let $\lambda$ be the product of
the measures $\mu\restr\Sigma_i$, and let $\phi:\Omega\to\Omega^I$ be
the diagonal map, as in 272J.   $\phi$ is \imp\ for $\hat\mu$ and
$\lambda$, by 272J.

We can identify $\lambda$ with the product of
$\langle\lambda_s\rangle_{s\in S}$, where for each $s\in S\,\,\lambda_s$
is the product of $\langle\mu\restr\Sigma_i\rangle_{i\in J(s)}$
(254N).  For $s\in S$, let $\Lambda_s$ be the domain of $\lambda_s$, and
set $\pi_s(x)=x\restr J(s)$ for $x\in\Omega^I$, so that $\pi_s$ is \imp\
for $\lambda$ and $\lambda_s$ (254Oa), and $\phi_s=\pi_s\phi$ is \imp\
for $\hat\mu$ and $\lambda_s$;  of course $\phi_s$ is the diagonal map
from $\Omega$ to $\Omega^{J(s)}$.   Set
$\Sigma_s^*=\{\phi_s^{-1}[H]:H\in\Lambda_s\}$.   Then $\Sigma_s^*$ is a
$\sigma$-subalgebra of $\hat\Sigma$, and
$\Sigma_s^*\supseteq\tilde\Sigma_s$, because

\Centerline{$E=\phi_s^{-1}[\{x:x(i)\in E\}]\in\Sigma^*_s$}

\noindent whenever $i\in J(s)$a dn $E\in\Sigma_i$.

Now suppose that $s_1,\ldots,s_n\in S$ are distinct and that
$E_j\in\tilde\Sigma_{s_j}$ for each $j$.   Then $E_j\in\Sigma_{s_j}^*$,
so there are $H_j\in\Lambda_{s_j}$ such that $E_j=\phi_{s_j}^{-1}[H_j]$
for each $j$.   Set

\Centerline{$W=\{x:x\in\Omega^I,\,x\restr J(s_j)\in H_j$ for every $j\le
n\}$.}

\noindent Because we can identify $\lambda$ with the product of the
$\lambda_s$, we have

\Centerline{$\lambda W=\prod_{j=1}^n\lambda_{s_j}H_j
=\prod_{j=1}^n\hat\mu(\phi_{s_j}^{-1}[H_j])
=\prod_{j=1}^n\hat\mu E_j=\prod_{j=1}^n\mu E_j$.}

\noindent On the other hand, $\phi^{-1}[W]=\bigcap_{j\le n}E_j$, so,
because $\phi$ is \imp,

\Centerline{$\mu(\bigcap_{j\le n}E_j)=\hat\mu(\bigcap_{j\le n}E_j)
=\lambda W=\prod_{j=1}^n\mu E_j$.}

\noindent As $E_1,\ldots,E_n$ are arbitrary,
$\langle\tilde\Sigma_s\rangle_{s\in S}$ is independent.
}%end of proof of 272K

\leader{272L}{}\cmmnt{ I give a typical application of this result as
a sample.

\medskip

\noindent}{\bf Corollary} Let $X,X_1,\ldots,X_n$ be independent
real-valued random
variables and $h:\BbbR^n\to\Bbb R$ a Borel 
measurable function.   Then $X$ and $h(X_1,\ldots,X_n)$ are independent.

\proof{ Let $\Sigma_X$, $\Sigma_{X_i}$ be the
$\sigma$-algebras defined by $X$, $X_i$ (272C).   Then
$\Sigma_X,\Sigma_{X_1},\ldots,\Sigma_{X_n}$ are independent (272D).
Let $\Sigma^*$ be the $\sigma$-algebra generated by
$\Sigma_{X_1}\cup\ldots\cup\Sigma_{X_n}$.   Then 272K (perhaps working
in the completion of the original probability space) tells us that
$\Sigma_X$ and $\Sigma^*$ are independent.   But every $X_j$ is
$\Sigma^*$-measurable so $Y=h(X_1,\ldots,X_n)$ is $\Sigma^*$-measurable
(121Kb);  also $\dom Y\in\Sigma^*$, so $\Sigma_Y\subseteq\Sigma^*$ and
$\Sigma_X$, $\Sigma_Y$ are independent.   By 272D again, $X$ and $Y$ are
independent, as claimed.
}%end of proof of 272L

\cmmnt{\medskip

\noindent{\bf Remark} Nearly all of us, when teaching elementary
probability theory, would invite our students to treat this corollary
(with an explicit function $h$, of course) as `obvious'.   In effect,
the proof here is a
confirmation that the formal definition of `independence' offered is
a faithful representation of our intuition of independent events having
independent causes.
}%end of comment

\leader{272M}{}{\bf Products of probability spaces and independent
families of
random \dvrocolon{variables}}\cmmnt{ We have already seen that
the concept of
`independent random variables' is intimately linked with that of
`product measure'.   I now give some further manifestations of the
connexion.

\medskip

\noindent}{\bf Proposition} Let
$\langle(\Omega_i,\Sigma_i,\mu_i)\rangle_{i\in I}$ be a family of
probability spaces, and $(\Omega,\Sigma,\mu)$ their product.

(a) For each $i\in I$ write
$\tilde\Sigma_i=\{\pi_i^{-1}[E]:E\in\Sigma_i\}$, where
$\pi_i:\Omega\to\Omega_i$ is the coordinate map.   Then
$\langle\tilde\Sigma_i\rangle_{i\in I}$ is an independent family of
$\sigma$-subalgebras of $\Sigma$.

(b) For each $i\in I$ let $\langle X_{ij}\rangle_{j\in J(i)}$ be an
independent family of real-valued random variables on
$\Omega_i$, and for $i\in I$, $j\in J(i)$ write $\tilde
X_{ij}(\omega)=X_{ij}(\omega(i))$ for those
$\omega\in\Omega$ such that $\omega(i)\in\dom X_{ij}$.   Then
$\langle \tilde X_{ij}\rangle_{i\in I,j\in J(i)}$ is an independent
family of
random variables, and each $\tilde X_{ij}$ has the same distribution as
the corresponding $X_{ij}$.

\proof{{\bf (a)}  It is easy to check that each $\tilde\Sigma_i$ is a
$\sigma$-algebra of sets.   The rest amounts just to recalling from
254Fb that if $J\subseteq I$ is finite and $E_i\in\Sigma_{i}$ for
$i\in J$, then

\Centerline{$\mu(\bigcap_{i\in J}\pi_i^{-1}[E_i])
=\mu\{\omega:\omega(i)\in E_i\text{ for every }i\in I\}
=\prod_{i\in I}\mu_{i}E_i$}

\noindent if we set $E_i=X_i$ for $i\in I\setminus J$.

\medskip

{\bf (b)} We know also that $(\Omega,\Sigma,\mu)$ is the product of the
completions $(\Omega_i,\hat\Sigma_i,\hat\mu_i)$ (254I).
From this, we see that each $\tilde X_{ij}$ is defined $\mu$-a.e., and
is $\Sigma$-measurable, with the same distribution as $X_{ij}$.    Now
apply condition (iii) of 272D.   Suppose that $\langle
F_{ij}\rangle_{i\in I,j\in J(i)}$ is a family of Borel sets in $\Bbb R$,
and that $\{(i,j):F_{ij}\ne\Bbb R\}$ is finite.   Consider

\Centerline{$E_i
=\bigcap_{j\in J(i)}
(X_{ij}^{-1}[F_{ij}]\cup(\Omega_i\setminus\dom X_{ij}))$,}

\Centerline{$E=\prod_{i\in I}E_i
=\bigcap_{i\in I,j\in J(i)}
(\tilde X_{ij}^{-1}[F_{ij}]\cup(\Omega\setminus\dom \tilde X_{ij}))$.}

\noindent Because each family $\langle X_{ij}\rangle_{j\in J(i)}$ is
independent, and $\{j:F_{ij}\ne\Bbb R\}$ is finite,

\Centerline{$\hat\mu_iE_i=\prod_{j\in J(i)}\Pr(X_{ij}\in E_{ij})$}

\noindent for each $i\in I$.   Because

\Centerline{$\{i:E_i\ne \Omega_i\}\subseteq\{i:\exists\,j\in
J(i),\,F_{ij}\ne\Bbb R\}$}

\noindent is finite,

\Centerline{$\mu E=\prod_{i\in I}\hat\mu_iE_i
=\prod_{i\in I,j\in J(i)}\Pr(\tilde X_{ij}\in F_{ij})$;}

\noindent as $\langle F_{ij}\rangle_{i\in I,j\in J(i)}$ is arbitrary,
$\langle \tilde X_{ij}\rangle_{i\in I,j\in J(i)}$ is independent.
}%end of proof of 272M

\cmmnt{\medskip

\noindent{\bf Remark} The formulation in (b) is more complicated than is
necessary to express the idea, but is what is needed for an application
below.
}

\leader{272N}{}\cmmnt{ A special case of 272J is  of particular
importance in general measure theory, and is most useful in an adapted
form.

\medskip

\noindent}{\bf Proposition} Let $(\Omega,\Sigma,\mu)$ be a complete
probability space, and $\langle E_i\rangle_{i\in I}$ an independent
family in $\Sigma$ such that $\mu E_i=\bover12$ for every $i\in I$.
Define $\phi:\Omega\to\{0,1\}^I$ by setting $\phi(\omega)(i)=1$ if
$\omega\in E_i$, $0$ if $\omega\in\Omega\setminus E_i$.   Then $\phi$ is
inverse-measure-preserving for the usual measure $\lambda$ on
$\{0,1\}^I$\cmmnt{ (254J)}.

\proof{ I use 254G again.   For each $i\in I$ let $\Sigma_i$ be the
algebra $\{\emptyset,E_i,\Omega\setminus E_i,\Omega\}$;  then
$\langle\Sigma_i\rangle_{i\in I}$ is independent (272F).   For $i\in I$
set $\phi_i(\omega)=\phi(\omega)(i)$.   Let $\nu$ be the usual measure
of $\{0,1\}$.   Then it is easy to check that

\Centerline{$\mu\phi_i^{-1}[H]=\Bover12\#(H)=\nu H$}

\noindent for every $H\subseteq\{0,1\}$.
If $\langle H_i\rangle_{i\in I}$ is a family of subsets of $\{0,1\}$,
and $\{i:H_i\ne\{0,1\}\}$ is finite, then

$$\eqalignno{\mu\phi^{-1}[\bigcap_{i\in I}H_i]
&=\mu(\bigcap_{i\in I}\phi_i^{-1}[H_i])
=\prod_{i\in J}\mu\phi_i^{-1}[H_i]\cr
\noalign{\noindent (because $\phi^{-1}[H_i]\in\Sigma_i$ for each $i$,
and $\langle\Sigma_i\rangle_{i\in I}$ is independent)}
&=\prod_{i\in I}\nu H_i
=\lambda(\prod_{i\in I}H_i).\cr}$$

\noindent As $\langle H_i\rangle_{i\in I}$ is arbitrary, 254G gives the
result.
}%end of proof of 272N

\leader{272O}{Tail $\sigma$-algebras and the
zero-one \dvrocolon{law}}\cmmnt{ I have never
been able to make up my mind whether the following result is `deep'
or not.   I think it is one of the many cases in mathematics where a
theorem is surprising and exciting if one comes on it unprepared, but is
natural and straightforward if one approaches it from the appropriate
angle.

\medskip

\noindent}{\bf Proposition} Let $(\Omega,\Sigma,\mu)$ be a probability
space and
$\sequencen{\Sigma_n}$ an independent sequence of $\sigma$-subalgebras
of $\Sigma$.   Let $\Sigma_n^*$ be the $\sigma$-algebra generated by
$\bigcup_{m\ge n}\Sigma_m$ for each $n$, and set
$\Sigma_{\infty}^*=\bigcap_{n\in\Bbb N}\Sigma_n^*$.   Then $\mu E$
is either $0$ or $1$ for every $E\in\Sigma_{\infty}^*$.

\proof{ For each $n$, the family
$(\Sigma_0,\ldots,\Sigma_n,\Sigma^*_{n+1})$ is independent, by 272K.
So $(\Sigma_0,\ldots,\Sigma_n,\Sigma^*_{\infty})$ is independent,
because $\Sigma^*_{\infty}\subseteq\Sigma^*_{n+1}$.   But this means
that every finite subfamily of
$(\Sigma^*_{\infty},\Sigma_0,\Sigma_1,\ldots)$ is independent, and
therefore that the whole family is (272Bb).   Consequently
$(\Sigma^*_{\infty},\Sigma^*_0)$ must be independent, by 272K again.

Now if $E\in\Sigma^*_{\infty}$, then $E$ also belongs to $\Sigma^*_0$,
so we must have

\Centerline{$\mu(E\cap E)=\mu E\cdot\mu E$,}

\noindent that is, $\mu E=(\mu E)^2$;  so that $\mu E\in\{0,1\}$, as
claimed.
}%end of proof of 272O

\leader{272P}{}\cmmnt{ To support the claim that somewhere we have
achieved a
non-trivial insight, I give a corollary, which will be fundamental to
the understanding of the limit theorems in the next section, and does
not seem to be obvious.

\medskip

\noindent}{\bf Corollary} Let $(\Omega,\Sigma,\mu)$ be a probability
space, and $\langle X_n\rangle_{n\in\Bbb N}$ an independent sequence of
real-valued random variables on $\Omega$.   Then

\Centerline{$\limsup_{n\to\infty}\Bover{1}{n+1}(X_0+\ldots+X_n)$}

\noindent is almost everywhere constant\dvro{.}{}\cmmnt{ -- that is,
there is some $u\in[-\infty,\infty]$ such that

\Centerline{$\limsup_{n\to\infty}
\Bover{1}{n+1}(X_0+\ldots+X_n)=u$}

\noindent almost everywhere.
}%end of comment

\proof{ We may suppose that each $X_n$ is $\Sigma$-measurable and
defined everywhere in
$\Omega$,  because (as remarked in 272H) changing the $X_n$ on a
negligible set does not affect their independence, and it affects
$\limsup_{n\to\infty}\Bover1{n+1}(X_0+\ldots+X_n)$ only on a
negligible set.   For each $n$, let $\Sigma_n$ be the
$\sigma$-algebra generated by $X_n$ (272C), and $\Sigma_n^*$ the
$\sigma$-algebra generated by $\bigcup_{m\ge n}\Sigma_m$;  set
$\Sigma_{\infty}^*=\bigcap_{n\in\Bbb N}\Sigma_n^*$.   By 272D,
$\sequencen{\Sigma_n}$ is independent, so $\mu E\in\{0,1\}$ for
every $E\in\Sigma_{\infty}^*$ (272O).

Now take any $a\in\Bbb R$ and set

\Centerline{$E_a=\{\omega:
\limsup_{m\to\infty}\Bover1{m+1}(X_0(\omega)+\ldots+X_m(\omega))
\le a\}$.}

\noindent Then

\Centerline{$\limsup_{m\to\infty}\Bover1{m+1}(X_0+\ldots+X_m)
=\limsup_{m\to\infty}\Bover1{m+1}(X_n+\ldots+X_{m+n})$,}

\noindent so


\Centerline{$E_a
=\{\omega:\limsup_{m\to\infty}
\Bover1{m+1}(X_n(\omega)+\ldots+X_{n+m}(\omega))\le a\}$}

\noindent belongs to $\Sigma^*_n$ for every $n$, because $X_i$ is
$\Sigma_n^*$-measurable for every $i\ge n$.   So $E\in\Sigma_{\infty}^*$
and

\Centerline{$\Pr(\limsup_{m\to\infty}
  \Bover1{m+1}(X_0+\ldots+X_m)\le a)
=\mu E_a$}

\noindent must be either $0$ or $1$.   Setting

\Centerline{$u=\sup\{a:a\in\Bbb R,\,\mu E_a=0\}$}

\noindent (allowing $\sup\emptyset=-\infty$ and $\sup\Bbb R=\infty$, as
usual in such contexts), we see that

\Centerline{$\limsup_{n\to\infty}\Bover1{n+1}(X_0+\ldots+X_n)=u$}

\noindent almost everywhere.
}%end of proof of 272P

\leader{*272Q}{}\cmmnt{ I add here a result which will be useful in
Volume 5 and
which gives further insight into the nature of large independent families.

\medskip

\noindent}{\bf Theorem}\dvAnew{2009.}
Let $(\Omega,\Sigma,\mu)$ be a probability space,
and $\familyiI{\Sigma_i}$ an independent family of $\sigma$-subalgebras of
$\Sigma$.   Let $\Cal E\subseteq\Sigma$ be a family of measurable sets, and
$\Tau$ the $\sigma$-algebra generated by $\Cal E$.   Then there is a set
$J\subseteq I$ such that $\#(I\setminus J)\le\max(\omega,\#(\Cal E))$ and
$\Tau$, $\family{j}{J}{\Sigma_j}$ are independent, in the sense that
$\mu(F\cap\bigcap_{r\le n}E_r)=\mu F\cdot\prod_{r=1}^n\mu E_r$ whenever
$F\in\Tau$, $j_1,\ldots,j_r$ are distinct members of $J$ and
$E_r\in\Sigma_{j_r}$ for each $r\le n$.

\proof{{\bf (a)} As in 272J, give $\Omega^I$ the probability measure
$\lambda$ which is
the product of the measures $\mu\restr\Sigma_i$, and let $\phi:\Omega\to \Omega^I$
be the diagonal map, so that $\phi$ is \imp\ for $\hat\mu$
and $\lambda$, where $\hat\mu$ is the completion of $\mu$.
Write $\Lambda$ for the domain of $\lambda$.   Set
$\kappa=\max(\omega,\#(\Cal E))$, and let $\Cal E^*$ be the set
$\{\bigcap_{r\le n}F_r:n\in\Bbb N$, $F_r\in\Cal E$ for every $r\le n\}$.
Because $\#(\Cal E^n)\le\kappa$ for each $n$ (2A1Lc),
$\#(\Cal E^*)\le\kappa$ (2A1Ld).   For each
$F\in\Cal E^*$, define $\nu_F:\Lambda\to[0,1]$ by setting
$\nu_FW=\hat\mu(F\cap\phi^{-1}[W])$;  then $\nu_F$ is countably additive
and dominated by $\lambda$.
It therefore has a Radon-Nikod\'ym derivative $h_F$ with respect to
$\lambda$, so that $\hat\mu(F\cap\phi^{-1}[W])=\int_Wh_Fd\lambda$ for
every $W\in\Lambda$ (232F).   By 254Qc or 254Rb, we can find a function
$h'_F$ equal $\lambda$-almost everywhere to $h_F$ and determined by
coordinates in a countable set $J_F$, in the sense that $h'_F(w)=h'_F(w')$
whenever $w$, $w'\in \Omega^I$ and $w\restr J_F=w'\restr J_F$.   (I am taking it
for granted that we chose $h'_F$ to be defined everywhere on $\Omega^I$.)

\medskip

{\bf (b)} Set $J=I\setminus\bigcup_{F\in\Cal E^*}J_F$;  by 2A1Ld,
$I\setminus J=\bigcup_{F\in\Cal E^*}J_F$ has cardinal at most $\kappa$.
If $F\in\Cal E^*$, $j_1,\ldots,j_r$ are distinct members of $J$ and
$E_r\in\Sigma_{j_r}$ for each $r\le n$,
$\mu(F\cap\bigcap_{r\le n}E_r)=\mu F\cdot\prod_{r=1}^n\mu E_r$.   \Prf\
Set  $W=\{w:w\in \Omega^I$, $w(j_r)\in E_r$ for each $r\le n\}$.   Then

\Centerline{$\mu(F\cap\bigcap_{r\le n}E_r)
=\hat\mu(F\cap\phi^{-1}[W])
=\int_Wh'_Fd\lambda
=\int h'_F\times\chi W\,d\lambda$.}

\noindent But observe that $W$ is determined by coordinates in $J$, while
$h'_F$ is determined by coordinates in $J_F\subseteq I\setminus J$;
putting 272Ma, 272K and 272R together (or otherwise), we have

\Centerline{$\mu(F\cap\bigcap_{r\le n}E_r)
=\int h'_F\times\chi W\,d\lambda
=\int h'_Fd\lambda\cdot\lambda W
=\mu F\cdot\prod_{r=1}^n\mu E_r$.  \Qed}

\medskip

{\bf (c)} Now
consider the family $\Cal A$ of those sets $F\in\Sigma$ such that
$\mu(F\cap\bigcap_{r\le n}E_r)
=\mu F\cdot\prod_{r=1}^n\mu E_r$ whenever $j_1,\ldots,j_n\in J$ are
distinct and $E_r\in\Sigma_{j_r}$ for every $r\le n$.   It is easy to check
that $\Cal A$ is a Dynkin class, and we have just seen that
$\Cal A$ includes $\Cal E^*$;  as $\Cal E^*$ is closed under $\cap$,
$\Cal A$ includes the $\sigma$-algebra $\Tau$ of sets generated by
$\Cal E^*$ (136B).   And this is just what the theorem asserts.
}%end of proof of 272Q

\leader{272R}{}\dvAformerly{2{}72Q}\cmmnt{ I must
now catch up on some basic facts from elementary probability theory.

\medskip

\noindent}{\bf Proposition} Let $X$, $Y$ be independent
real-valued random variables with finite
expectation\cmmnt{ (271Ab)}.   Then
$\Expn(X\times Y)$ exists and is equal to $\Expn(X)\Expn(Y)$.

\proof{ Let $\nu_{(X,Y)}$ be the joint distribution of the pair $(X,Y)$.
Then $\nu_{(X,Y)}$ is the product of the distributions $\nu_X$ and
$\nu_Y$ (272G).   Also $\int x\nu_X(dx)=\Expn(X)$ and
$\int y\nu_Y(dy)=\Expn(Y)$ exist in $\Bbb R$ (271F).   So

\Centerline{$\int xy\nu_{(X,Y)}d(x,y)$ exists $=\Expn(X)\Expn(Y)$}

\noindent (253D).   But this is just $\Expn(X\times Y)$, by 271E with
$h(x,y)=xy$.
}%end of proof of 272R

\leader{272S}{Bienaym\'e's Equality}\dvAformerly{2{}72R} Let
$X_1,\ldots,X_n$ be independent
real-valued random variables.   Then
$\Var(X_1+\ldots+X_n)=\Var(X_1)+\ldots+\Var(X_n)$.

\proof{{\bf (a)} Suppose first that all the $X_i$ have finite variance.
Set $a_i=\Expn(X_i)$, $Y_i=X_i-a_i$, $X=X_1+\ldots+X_n$,
$Y=Y_1+\ldots+Y_n$;  then $\Expn(X)=a_1+\ldots+a_n$, so $Y=X-\Expn(X)$
and

$$\eqalignno{\Var(X)
&=\Expn(Y^2)
=\Expn(\sum_{i=1}^n Y_i)^2\cr
&=\Expn(\sum_{i=1}^n\sum_{j=1}^n Y_i\times Y_j)
=\sum_{i=1}^n\sum_{j=1}^n\Expn(Y_i\times Y_j).\cr}$$

\noindent Now observe that if $i\ne j$ then
$\Expn(Y_i\times Y_j)=\Expn(Y_i)\Expn(Y_j)=0$, because $Y_i$ and $Y_j$
are independent
(by 272E) and we may use 272R, while if $i=j$ then

\Centerline{$\Expn(Y_i\times Y_j)=\Expn(Y_i^2)=\Expn(X_i-\Expn(X_i))^2
=\Var(X_i)$.}

\noindent So

\Centerline{$\Var(X)=\sum_{i=1}^n\Expn(Y_i^2)=\sum_{i=1}^n\Var(X_i)$.}

\medskip

{\bf (b)(i)} I show next that if $\Var(X_1+X_2)<\infty$ then
$\Var(X_1)<\infty$.   \Prf\ We have

$$\eqalignno{\iint(x+y)^2\nu_{X_1}(dx)\nu_{X_2}(dy)
&=\int (x+y)^2\nu_{(X_1,X_2)}(d(x,y))\cr
\noalign{\noindent (by 272G and Fubini's theorem)}
&=\Expn((X_1+X_2)^2)\cr
\noalign{\noindent (by 271E)}
&<\infty.\cr}$$

\noindent So there must be some $a\in\Bbb R$ such that 
$\int(x+a)^2\mu_{X_1}(dx)$ is finite, that is, $\Expn((X_1+a)^2)<\infty$;
consequently $\Expn(X_1^2)$ and $\Var(X_1)$ are finite.   \Qed

\medskip

\quad{\bf (ii)} Now an easy induction (relying on 272L!) shows that if
$\Var(X_1+\ldots+X_n)$ is finite, so is $\Var X_j$ for every $j$.
Turning this round, if $\sum_{j=1}^n\Var(X_j)=\infty$, then
$\Var(X_1+\ldots+X_n)=\infty$, and again the two are equal.
}%end of proof of 272S

\leader{272T}{The distribution of a sum of independent random variables:
Theorem}\dvAformerly{2{}72S} Let
$X$, $Y$ be independent real-valued random variables on a
probability space $(\Omega,\Sigma,\mu)$, with distributions $\nu_X$,
$\nu_Y$.   Then the distribution of $X+Y$ is the convolution
$\nu_X*\nu_Y$\cmmnt{ (257A)}.

\wheader{272T}{0}{0}{0}{48pt}

\proof{ Set $\nu=\nu_X*\nu_Y$.   Take $a\in\Bbb R$ and set
$h=\chi\ocint{-\infty,a}$.   Then $h$ is $\nu$-integrable, so

$$\eqalignno{\nu\ocint{-\infty,a}
&=\int h\,d\nu
=\int h(x+y)(\nu_X\times\nu_Y)(d(x,y))\cr
\noalign{\noindent (by 257B, writing $\nu_X\times\nu_Y$ for the product
measure on $\BbbR^2$)}
&=\int h(x+y)\nu_{(X,Y)}(d(x,y))\cr
\noalign{\noindent (by 272G, writing $\nu_{(X,Y)}$ for the joint
distribution of $(X,Y)$;  this is where we use the hypothesis that $X$
and $Y$ are independent)}
&=\Expn(h(X+Y))\cr
\noalign{\noindent (applying 271E to the function
$(x,y)\mapsto h(x+y)$)}
&=\Pr(X+Y\le a).\cr}$$

\noindent As $a$ is arbitrary, $\nu_X*\nu_Y$ is the distribution of
$X+Y$.
}%end of proof of 272T

\leader{272U}{Corollary}\dvAformerly{2{}72T} Suppose
that $X$ and $Y$ are independent real-valued random
variables, and that they have densities $f$ and $g$.   Then the convolution
$f*g$ is a density function for $X+Y$.

\proof{ By 257F, $f*g$ is a density function for
$\nu_X*\nu_Y=\nu_{X+Y}$.
}%end of proof of 272U

\leader{272V}{}\dvAformerly{2{}72U}\cmmnt{ The following simple result
will be very useful when we come to stochastic processes in Volume 4,
as well as in the next section.

\medskip

\noindent}{\bf Etemadi's lemma}\cmmnt{ ({\smc Etemadi 96})} Let
$X_0,\ldots,X_n$ be independent real-valued random variables.   For
$m\le n$, set $S_m=\sum_{i=0}^mX_i$.   Then

\Centerline{$\Pr(\sup_{m\le n}|S_m|\ge 3\gamma)
\le 3\max_{m\le n}\Pr(|S_m|\ge\gamma)$}

\noindent for every $\gamma>0$.

\proof{ As in 272P, we may suppose that every $X_i$ is a measurable
function defined everywhere on a measure space $\Omega$.   Set
$\alpha=\max_{m\le n}\Pr(|S_m|\ge\gamma)$.   For each $r\le n$, set

\Centerline{$E_r=\{\omega:|S_m(\omega)|<3\gamma$ for every $m<r$,
$|S_r(\omega)|\ge 3\gamma\}$.}

\noindent Then $E_0,\ldots,E_n$ is a partition of
$\{\omega:\max_{m\le n}|S_m(\omega)|\ge 3\gamma\}$.   Set
$E'_r=\{\omega:\omega\in E_r$, $|S_n(\omega)|<\gamma\}$.   Then
$E'_r\subseteq\{\omega:\omega\in E_r$, $|(S_n-S_r)(\omega)|>2\gamma\}$.
But $E_r$ depends on $X_0,\ldots,X_r$ so is independent of
$\{\omega:|(S_n-S_r)(\omega)|>2\gamma\}$, which can be calculated
from $X_{r+1},\ldots,X_n$ (272K).   So

$$\eqalign{\mu E'_r
&\le\mu\{\omega:\omega\in E_r,\,|(S_n-S_r)(\omega)|>2\gamma\}
=\mu E_r\cdot\Pr(|S_n-S_r|>2\gamma)\cr
&\le\mu E_r(\Pr(|S_n|>\gamma)+\Pr(|S_r|>\gamma))
\le 2\alpha\mu E_r,\cr}$$

\noindent and $\mu(E_r\setminus E'_r)\ge(1-2\alpha)\mu E_r$.   On the
other hand, $\langle E_r\setminus E'_r\rangle_{r\le n}$ is a disjoint
family of sets all included in $\{\omega:|S_n(\omega)|\ge\gamma\}$.
So

\Centerline{$\alpha
\ge\mu\{\omega:|S_n(\omega)|\ge\gamma\}
\ge\sum_{r=0}^n\mu(E_r\setminus E'_r)
\ge(1-2\alpha)\sum_{r=0}^n\mu E_r$,}

\noindent and

\Centerline{$\Pr(\sup_{r\le n}|S_r|\ge 3\gamma)
=\sum_{r=0}^n\mu E_r
\le\min(1,\Bover{\alpha}{1-2\alpha})
\le 3\alpha$,}

\noindent (considering $\alpha\le\bover13$, $\alpha\ge\bover13$
separately), as required.
}%end of proof of 272V

\leader{*272W}{}\dvAnew{2009.}\cmmnt{ The next result is a similarly
direct application
of the ideas of this section.   While it will not be used in this volume,
it is an accessible and useful representative
of a very large number of results on tails of sums of independent random
variables.

\medskip

\noindent}{\bf Theorem}\cmmnt{ ({\smc Hoeffding 63})}
Let $X_0,\ldots,X_n$ be
independent real-valued random variables such that $0\le X_i\le 1$ a.e.\
for every $i$.
Set $S=\bover1{n+1}\sum_{i=0}^nX_i$ and $a=\Expn(S)$.   Then

\Centerline{$\Pr(S-a\ge c)\le\exp(-2(n+1)c^2)$}

\noindent for every $c\ge 0$.

\proof{{\bf (a)} Set $a_i=\Expn(X_i)$ for each $i$.
If  $b\ge 0$ and $i\le n$, then

\Centerline{$\Expn(e^{bX_i})\le\exp(ba_i+\Bover18b^2)$.}

\noindent\Prf\
Set $\phi(x)=e^{bx}$ for $x\in\Bbb R$.   Then $\phi$ is
convex, so

\Centerline{$\phi(x)
\le 1+x(e^b-1)$}

\noindent whenever $x\in[0,1]$,

\Centerline{$\phi(X_i)
\leae 1+(e^b-1)X_i$}

\noindent and

\Centerline{$\Expn(e^{bX_i})
=\Expn(\phi(X_i))
\le 1+(e^b-1)a_i
=e^{h(b)}$}

\noindent where $h(t)=\ln(1-a_i+a_ie^t)$
for $t\in\Bbb R$.   Now $h(0)=0$,

\Centerline{$h'(t)=\Bover{a_ie^t}{1-a_i+a_ie^t}
=1-\Bover{1-a_i}{1-a_i+a_ie^t}$,
\quad$h'(0)=a_i$,}

\Centerline{$h''(t)
=\Bover{1-a_i}{1-a_i+a_ie^t}\cdot\Bover{a_ie^t}{1-a_i+a_ie^t}
\le\Bover14$}

\noindent because $a_ie^t$ and $1-a_i$ are both greater than or equal to
$0$.   By Taylor's theorem with remainder, there is some
$t\in[0,b]$ such that

\Centerline{$h(b)
=h(0)+bh'(0)+\Bover12b^2h_i''(t)
\le ba_i+\Bover18b^2$,}

\noindent and

\Centerline{$\Expn(e^{bX_i})\le\exp(ba_i+\Bover18b^2)$.  \Qed}

\medskip

{\bf (b)} Take any $b\ge 0$.   Then

$$\eqalignno{\Pr(S-a\ge c)
&=\Pr(\sum_{i=0}^n(X_i-a_i-c)\ge 0)
\le\Expn(\exp(b\sum_{i=0}^nX_i-a_i-c))\cr
\displaycause{because $\exp(b\sum_{i=0}^nX_i-a_i-c)\ge 1$ whenever
$\sum_{i=0}^nX_i-a_i-c\ge 0$}
&=e^{-(n+1)bc}\prod_{i=0}^ne^{-ba_i}\Expn(\prod_{i=0}^n\exp(bX_i))\cr
&=e^{-(n+1)bc}\prod_{i=0}^ne^{-ba_i}
  \prod_{i=0}^n\Expn(\exp(bX_i))\cr
\displaycause{because the random variables $\exp(bX_i)$ are independent,
by 272E, so
the expectation of the product is the product of the expectations, by
272R}
&\le e^{-(n+1)bc}\prod_{i=0}^ne^{-ba_i}\exp(ba_i+\Bover18b^2)\cr
\displaycause{(a) above}
&=\exp(-(n+1)bc+\Bover{n+1}8b^2).\cr}$$

\noindent Now the minimum value of the quadratic $\Bover{n+1}8b^2-(n+1)cb$
is $-2(n+1)c^2$ when $b=4c$, so $\Pr(S-a\ge c)\le\exp(-2(n+1)c^2)$, as
claimed.
}%end of proof of 272W

\exercises{
\leader{272X}{Basic exercises (a)}
%\spheader 272Xa
Let $(\Omega,\Sigma,\mu)$ be an atomless probability space, and
$\sequencen{\epsilon_n}$ any sequence in $[0,1]$.   Show that there is
an independent sequence $\sequencen{E_n}$ in $\Sigma$ such that
$\mu E_n=\epsilon_n$ for every $n$.   \Hint{215D.}
%272B

\sqheader 272Xb Let $\langle X_i\rangle_{i\in I}$ be a family of
real-valued random variables.   Show that it is independent iff

\Centerline{$\Expn(h_1(X_{i_1})\times\ldots\times h_n(X_{i_n}))
=\prod_{j=1}^n\Expn(h_j(X_{i_j}))$}

\noindent whenever $i_1,\ldots,i_n$ are distinct members of $I$ and
$h_1,\ldots,h_n$ are Borel measurable functions from $\Bbb R$ to
$\Bbb R$ such that $\Expn(h_j(X_{i_j}))$ are all finite.
%272E

\spheader 272Xc Write out a proof of 272F which does not use the
theory of product measures.
%272F

\spheader 272Xd Let $\pmb{X}=(X_1,\ldots,X_n)$ be a
family of real-valued
random variables all defined on the same probability space,
and suppose that $\pmb{X}$ has a density function $f$ expressible in the
form $f(\xi_1,\ldots,\xi_n)=f_1(\xi_1)f_2(\xi_2)\ldots f_n(\xi_n)$ for
suitable functions $f_1,\ldots,f_n$ of one real variable.   Show that
$X_1,\ldots,X_n$ are independent.
%272G

\spheader 272Xe Let $X_1$, $X_2$ be independent real-valued random
variables both with distribution $\nu$ and distribution function $F$.
Set $Y=\max(X_1,X_2)$.   Show that the distribution of $Y$ is absolutely
continuous with respect to $\nu$, with a Radon-Nikod\'ym derivative
$F+F^{-}$, where $F^{-}(x)=\lim_{t\uparrow x}F(t)$ for every
$x\in\Bbb R$.
%272G

\spheader 272Xf Use 254Sa and the idea of 272J to give another proof of
272O.
%272O

\spheader 272Xg Let $(\Omega,\Sigma,\mu)$ be a probability space and
$\sequencen{\Sigma_n}$ a non-decreasing sequence of $\sigma$-subalgebras
of $\Sigma$.   Let $\Sigma_{\infty}$ be the $\sigma$-algebra generated
by $\bigcup_{n\in\Bbb N}\Sigma_n$.   Let $\Tau$ be another
$\sigma$-subalgebra of $\Sigma$ such that $\Sigma_n$ and $\Tau$ are
independent for each $n$.   Show that $\Sigma_{\infty}$ and $\Tau$ are
independent.   \Hint{apply the Monotone Class Theorem to
$\{E:\mu(E\cap F)=\mu E\cdot\mu F$ for every $F\in\Tau\}$.}   Use this
to prove 272O.
%272O

\spheader 272Xh Let $\sequencen{X_n}$ be a sequence of
real-valued random variables and $Y$ a real-valued
random variable such that $Y$ and $X_n$ are independent for
each $n\in\Bbb N$.   Suppose that $\Pr(Y\in\Bbb N)=1$ and that
$\sum_{n=0}^{\infty}\Pr(Y\ge n)\Expn(|X_n|)$ is finite.   Set
$Z=\sum_{n=0}^YX_n$ (that is,
$Z(\omega)=\sum_{n=0}^{Y(\omega)}X_n(\omega)$ whenever $\omega\in\dom Y$
is such that $Y(\omega)\in\Bbb N$ and $\omega\in\dom X_n$ for every
$n\le Y(\omega)$).   (i) Show that
$\Expn(Z)=\sum_{n=0}^{\infty}\Pr(Y\ge n)\Expn(X_n)$.   \Hint{set
$X'_n(\omega)=X_n(\omega)$ if $Y(\omega)\ge n$, $0$ otherwise.}   (ii)
Show that if $\Expn(X_n)=\gamma$ for every $n\in\Bbb N$ then
$\Expn(Z)=\gamma\Expn(Y)$.   (This is {\bf Wald's equation}.)
%272R

\sqheader 272Xi Let $X_1,\ldots,X_n$ be independent real-valued random
variables.   Show that if $X_1+\ldots+X_n$ has finite expectation so
does every $X_j$.   \Hint{part (b) of the proof of 272S.}
%272S

\sqheader 272Xj Let $X$ and $Y$ be independent real-valued random
variables with densities $f$ and $g$.   Show that $X\times Y$ has
a density function $h$ where
$h(x)=\int_{-\infty}^{\infty}\bover1{|y|}g(y)f(\bover{x}{y})dy$ for
almost every $x$.   \Hint{271K.}
%272U

\spheader 272Xk\dvAnew{2009.} Let
$X_0,\ldots,X_n$ be independent real-valued
random variables such that $d_i\le X_i\le d'_i$ a.e.\
for every $i$.   (i) Show that if $b\ge 0$ then
$\Expn(e^{bX_i})\le\exp(ba_i+\Bover18b^2(d'_i-d_i)^2)$ for each $i$,
where $a_i=\Expn(X_i)$.   (ii)
Set $S=\bover1{n+1}\sum_{i=0}^nX_i$ and $a=\Expn(S)$.   Show that

\Centerline{$\Pr(S-a\ge c)\le\exp(-\Bover{2(n+1)^2c^2}{d})$}

\noindent for every $c\ge 0$, where $d=\sum_{i=0}^n(d'_i-d_i)^2$.
%272W

\spheader 272Xl\dvAnew{2009.} Suppose
that $X_0,\ldots,X_n$ are independent real-valued random
variables, all with expectation $0$, such that $\Pr(|X_i|\le 1)=1$ for
every $i$.   Set $S=\Bover1{\sqrt{n+1}}\sum_{i=0}^nX_i$.   Show that
$\Pr(S\ge c)\le\exp(-c^2/2)$ for every $c\ge 0$.
%272Xk 272W

\leader{272Y}{Further exercises (a)}
%\spheader 272Ya
\dvAformerly{2{}72Yc}
Let $X_0,\ldots,X_n$ be independent real-valued random
variables with distributions $\nu_0,\ldots,\nu_n$ and distribution
functions $F_0,\ldots,F_n$.   Show that, for any Borel set
$E\subseteq\Bbb R$,

\Centerline{$\Pr(\sup_{i\le n}X_i\in E)
=\sum_{i=0}^n\int_E\prod_{j=0}^{i-1}F_j^{-}(x)
  \prod_{j=i+1}^nF_j(x)\nu_i(dx)$,}

\noindent where $F_j^{-}(x)=\lim_{t\uparrow x}F_j(t)$ for each $j$, and
we interpret the empty products $\prod_{j=0}^{-1}F_j^{-}(x)$,
$\prod_{j=n+1}^nF_j(x)$ as $1$.
%272Xe, 272G

\spheader 272Yb\dvAformerly{2{}72Yf}
Let $\pmb{X}=\sequencen{X_n}$ be an independent
sequence of real-valued random variables on a complete probability space
$(\Omega,\Sigma,\mu)$.   Let $\Cal B$ be the Borel $\sigma$-algebra of
$\BbbR^{\Bbb N}$ (271Ya).   Let $\nu_{\pmb{X}}^{(\Cal B)}$ be the
probability measure with domain $\Cal B$ defined by setting
$\nu_{\pmb{X}}^{(\Cal B)}E=\mu\pmb{X}^{-1}[E]$ for every $E\in\Cal B$,
and write $\nu_{\pmb{X}}$ for the completion of $\nu_{\pmb{X}}^{(\Cal
B)}$.   Show that $\nu_{\pmb{X}}$ is just the product of the
distributions $\nu_{X_n}$.
%272G

\spheader 272Yc\dvAformerly{2{}72Ye}
Let $X_1,\ldots,X_n$ be real-valued random variables such
that for each $j<n$ the family

\Centerline{$(X_1,\ldots,X_j,-X_{j+1},\ldots,-X_n)$}

\noindent has the same joint distribution as the original family
$(X_1,\ldots,X_n)$.   Set $S_j=X_1+\ldots+X_j$ for each $j\le n$.   (i)
Show that for any $a\ge 0$

\Centerline{$\Pr(\sup_{1\le j\le n}|S_j|\ge a)
\le 2\Pr(|S_n|\ge a)$.}

\noindent \Hint{show that if
$E_j=\{\omega:\omega\in\bigcap_{i\le n}\dom X_i,\,|S_i(\omega)|<a$ for
$i<j,\,|S_j(\omega)|\ge a\}$ then
$\mu\{\omega:\omega\in E_j,\,
  |S_n(\omega)|\ge|S_j(\omega)|\}\ge\bover12\mu E_j$.}
(ii) Show that $\Expn(\sup_{j\le n}|S_j|)\le 2\Expn(|S_n|)$.  (iii)
Show that $\Expn(\sup_{i\le n}S_i^2)\le 2\Expn(S_n^2)$.
%272V

\spheader 272Yd\dvAnew{2009.}
Let $\sequence{i}{X_i}$ be an independent sequence of
real-valued random variables, and set $S_n=\sum_{i=0}^nX_i$ for each $n$.
Show that if $\sequencen{S_n}$ converges to $S$ in $\eusm L^0$
for the topology of convergence in measure,
then $\sequencen{S_n}$ converges to $S$ a.e.
%272V mt27bits

\spheader 272Ye\dvAformerly{2{}72Yd}
Let $(\Omega,\Sigma,\mu)$ be a probability space.

\quad (i) Let $\sequencen{E_n}$ be an independent sequence in $\Sigma$.
Show that for any real-valued random variable $X$ with finite
expectation,

\Centerline{$\lim_{n\to\infty}\int_{E_n}X\,d\mu-\mu E_n\Expn(X)=0$.}

\noindent \Hint{let $\Tau_0$ be the subalgebra of $\Sigma$
generated by $\{E_n:n\in\Bbb N\}$ and $\Tau$ the $\sigma$-subalgebra of
$\Sigma$ generated by $\{E_n:n\in\Bbb N\}$.   Start by considering
$X=\chi E$ for $E\in\Tau_0$ and then $X=\chi E$ for $E\in\Tau$.   Move
from $\eusm L^1(\mu\restrp\Tau)$ to $\eusm L^1(\mu)$ by using conditional
expectations.}

\quad (ii) Let $\sequencen{X_n}$ be a uniformly integrable independent
sequence of real-valued
random variables on $\Omega$.   Show that for any bounded
real-valued random variable $Y$,

\Centerline{$\lim_{n\to\infty}\Expn(X_n\times Y)-\Expn(X_n)\Expn(Y)=0$.}

\quad(iii) Suppose that $1<p\le\infty$ and set $q=p/(p-1)$ (taking $q=1$
if $p=\infty$).   Let $\sequencen{X_n}$ be an independent sequence of
real-valued
random variables with $\sup_{n\in\Bbb N}\|X_n\|_p<\infty$, and $Y$ a
real-valued random variable with $\|Y\|_q<\infty$.   Show that

\Centerline{$\lim_{n\to\infty}\Expn(X_n\times Y)-\Expn(X_n)\Expn(Y)=0$.}

\spheader 272Yf\dvAformerly{2{}72Yg}
Let $(\Omega,\Sigma,\mu)$ be a probability space and
$\sequencen{Z_n}$  a sequence of
random variables on $\Omega$ such that
$\Pr(Z_n\in\Bbb N)=1$ for each $n$, and $\Pr(Z_m=Z_n)=0$ for all
$m\ne n$.   Let $\sequencen{X_n}$ be a sequence of real-valued random
variables on $\Omega$, all with the same distribution $\nu$, and
independent of each other and the $Z_n$, in the sense that if $\Sigma_n$
is the $\sigma$-algebra defined by $X_n$, and $\Tau_n$ the
$\sigma$-algebra
defined by $Z_n$, and $\Tau$ is the $\sigma$-algebra generated by
$\bigcup_{n\in\Bbb N}\Tau_n$, then $(\Tau,\Sigma_0,\Sigma_1,\ldots)$ is
independent.   Set $Y_n(\omega)=X_{Z_n(\omega)}(\omega)$ whenever this
is defined, that is, $\omega\in\dom Z_n$, $Z_n(\omega)\in\Bbb N$ and
$\omega\in\dom X_{Z_n(\omega)}$.   Show that $\sequencen{Y_n}$ is an
independent sequence of random variables and that every $Y_n$ has the
distribution $\nu$.

\spheader 272Yg\dvAformerly{2{}72Yb}
Show that all the ideas of this section apply
equally to complex-valued random variables, subject to suitable
adjustments (to be devised).

\spheader 272Yh\dvAformerly{2{}72Ya}  Develop a theory of independence
for random variables taking values in $\BbbR^r$, following through as
many as possible of the ideas of this section.
}%end of exercises

\endnotes{
\Notesheader{272} This section is lengthy for two reasons:  I am trying
to pack in the basic results associated with one of the most fertile
concepts of mathematics, and it is hard to know where to stop;  and I am
trying to do this in language appropriate to abstract measure theory,
insisting on a variety of distinctions which are peripheral to the
essential ideas.   For while I am prepared to be flexible on the
question of whether the letter $X$ should denote a space or a function,
some of the applications of these results which are most important to me
are in contexts where we expect to be exactly clear what the domains of
our functions are.   Consequently it is necessary to form an opinion on
such matters as what the $\sigma$-algebra defined by a random variable
really is (272C).

The point of 272Q is that the family $\Cal E$ does not have to be related
in any way to the family $\familyiI{\Sigma_i}$, except, of course, that we
are dealing with measurable sets.   All we need to know is that $I$ should
be large compared with $\Cal E$;  for instance, that $\Cal E$ is countable
and $I$ is uncountable.   The family $\family{j}{J}{\Sigma_j}$ is now a
kind of `tail' of $\familyiI{\Sigma_i}$, safely independent of the `head'
$\sigma$-algebra generated by $\Cal E$.

Of course I should emphasize again that such proofs as those in
272R-272S are to be thought of as confirmations that we have a suitable
model of probability theory, rather than as reasons for believing the
results to be valid in statistical contexts.   Similarly, 272T-272U can
be approached by a variety of intuitions concerning discrete random
variables and random variables with continuous densities, and while the
elegant general results are delightful, they are more important to the
pure mathematician than to the statistician.    But I came to an odd
obstacle in the proof of 272S, when showing that if $X_1+\ldots+X_n$ has
finite variance then so does every $X_j$.   We have done enough measure
theory for this to be readily dealt with, but the connexion with
ordinary probabilistic intuition, both here and in 272Xi, remains
unclear to me.

There are four ideas in 272W worth storing for future use.
The first is the estimate

\Centerline{$\Expn(e^{bX_i})\le 1-a_i+e^ba_i$}

\noindent in part (a), a crude but effective way of using the hypothesis
that $X_i$ is bounded.   The second is the use of Taylor's theorem to
show that $1-a_i+e^ba_i\le\exp(a_i+\bover18b^2)$.
The third is the estimate

\Centerline{$\Pr(Y\ge 0)\le\Expn(e^{bY})$ if $b\ge 0$}

\noindent used in part (b);  and the fourth is 272R.
After this one need only be sufficiently determined to reach 272Xk.   But
even the special case 272W is both striking and useful.
}%end of notes

\discrpage


