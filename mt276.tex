\frfilename{mt276.tex}
\versiondate{16.4.13}
\copyrightdate{2000}

\def\chaptername{Probability theory}
\def\sectionname{Martingale difference sequences}

\newsection{276}

Hand in hand with the concept of `martingale' is that of `martingale
difference sequence' (276A), a direct generalization of the notion of
`independent sequence'.   In this section I collect results which can be
naturally expressed in terms of difference sequences, including yet
another strong law of large numbers (276C).   I end the section with a
proof of Koml\'os' theorem (276H).

\leader{276A}{Martingale difference sequences (a)}\cmmnt{ If
$\sequencen{X_n}$
is a martingale adapted to a sequence $\sequencen{\Sigma_n}$ of
$\sigma$-algebras, then we have

\Centerline{$\int_EX_{n+1}-X_n=0$}

\noindent whenever $E\in\Sigma_n$.}   Let us say that if
$(\Omega,\Sigma,\mu)$ is a probability space, with completion
$(\Omega,\hat\Sigma,\hat\mu)$, and $\sequencen{\Sigma_n}$ is a
non-decreasing sequence of $\sigma$-subalgebras of $\hat\Sigma$, then a
{\bf martingale difference sequence adapted to $\sequencen{\Sigma_n}$}
is a sequence $\sequencen{X_n}$ of real-valued random variables on
$\Omega$, all with finite expectation, such that (i)
$\dom X_n\in\Sigma_n$ and
$X_n$ is $\Sigma_n$-measurable, for each $n\in\Bbb N$ (ii)
$\int_EX_{n+1}=0$ whenever $n\in\Bbb N$ and $E\in\Sigma_n$.

\header{276Ab}{\bf (b)}
\cmmnt{ Evidently} $\sequencen{X_n}$ is a martingale difference
sequence adapted
to $\sequencen{\Sigma_n}$ iff $\sequencen{\sum_{i=0}^nX_i}$ is a
martingale adapted to $\sequencen{\Sigma_n}$.

\header{276Ac}{\bf (c)}
Just as in 275Cd, we can say that a sequence $\sequencen{X_n}$ is in
itself a {\bf martingale difference sequence} if
$\sequencen{\sum_{i=0}^nX_i}$ is a martingale\cmmnt{, that is, if
$\sequencen{X_n}$ is a martingale difference sequence adapted to
$\sequencen{\tilde\Sigma_n}$, where $\tilde\Sigma_n$ is the
$\sigma$-algebra generated by $\bigcup_{i\le n}\Sigma_{X_i}$}.

\header{276Ad}{\bf (d)} If $\sequencen{X_n}$ is a martingale difference
sequence then $\sequencen{a_nX_n}$ is a martingale difference sequence
for any real $a_n$.

\header{276Ae}{\bf (e)}  If $\sequencen{X_n}$ is a martingale difference
sequence and $X'_n\eae X_n$ for every $n$, then $\sequencen{X'_n}$ is
a martingale difference sequence.   \cmmnt{(Compare 275Ce.)}

\cmmnt{\header{276Af}{\bf (f)} Of course the most important example of
`martingale difference sequence' is that of 275Bb:  any independent
sequence of random variables with zero expectation is a martingale difference
sequence.   It turns out that some of the theorems of \S273 concerning
such independent sequences may be generalized to martingale difference
sequences.}

\leader{276B}{Proposition} Let $\sequencen{X_n}$ be a martingale
difference sequence such that $\sum_{n=0}^{\infty}\Expn(X_n^2)<\infty$.
Then $\sum_{n=0}^{\infty}X_n$ is defined, and finite, almost everywhere.

\proof{{\bf (a)} Let $(\Omega,\Sigma,\mu)$ be the
underlying probability space, $(\Omega,\hat\Sigma,\hat\mu)$ its
completion,  and $\sequencen{\Sigma_n}$ a non-decreasing sequence of
$\sigma$-subalgebras of $\hat\Sigma$ such that $\sequencen{X_n}$ is
adapted to $\sequencen{\Sigma_n}$.  Set
$Y_n=\sum_{i=0}^nX_i$ for each $n\in\Bbb N$.   Then $\sequencen{Y_n}$ is
a martingale adapted to $\sequencen{\Sigma_n}$.

\medskip

{\bf (b)} $\Expn(Y_{n}\times X_{n+1})=0$ for each $n$.   \Prf\ $Y_n$ is
a sum of random variables with finite variance, so
$\Expn(Y_n^2)<\infty$, by 244Ba;  it follows that
$Y_n\times X_{n+1}$ has finite expectation, by 244Eb.   Because
the constant function $\pmb{0}$ is a conditional expectation of
$X_{n+1}$ on $\Sigma_n$,

\Centerline{$\Expn(Y_n\times X_{n+1})=\Expn(Y_n\times\pmb{0})=0$,}

\noindent by 242L.\ \Qed

\medskip

{\bf (c)} It follows that $\Expn(Y_n^2)=\sum_{i=0}^n\Expn(X_i^2)$ for
every $n$.   \Prf\ Induce on $n$.   For the inductive step, we have

\Centerline{$\Expn(Y_{n+1}^2)=\Expn(Y_n^2+2Y_n\times X_{n+1}+X_{n+1}^2)
=\Expn(Y_n^2)+\Expn(X_{n+1}^2)$}

\noindent because, by (b), $\Expn(Y_n\times X_{n+1})=0$.\ \Qed\

\medskip

{\bf (d)} Of course

\Centerline{$\Expn(|Y_n|)
=\int|Y_n|\times\chi\Omega\le\|Y_n\|_2\|\chi\Omega\|_2
=\sqrt{\Expn(Y_n^2)}$,}

\noindent so

\Centerline{$\sup_{n\in\Bbb N}\Expn(|Y_n|)
\le\sup_{n\in\Bbb N}\sqrt{\Expn(Y_n^2)}
=\sqrt{\sumop_{i=0}^{\infty}\Expn(X_i^2)}<\infty$.}

\noindent By 275G, $\lim_{n\to\infty}Y_n$ is defined and finite
almost everywhere, that is, $\sum_{i=0}^{\infty}X_i$ is defined and
finite almost everywhere.
}%end of proof of 276B

\leader{276C}{The strong law of large numbers:  fourth form}  Let
$\sequencen{X_n}$ be a martingale difference sequence, and suppose that
$\sequencen{b_n}$ is a non-decreasing
sequence in $\ooint{0,\infty}$, diverging to $\infty$, such that
$\sum_{n=0}^{\infty}\Bover1{b_n^2}\Var(X_n)<\infty$.   Then

\Centerline{$\lim_{n\to\infty}\Bover{1}{b_n}
\sum_{i=0}^n X_i=0$}

\noindent almost everywhere.

\proof{ (Compare 273D.) As usual, write
$(\Omega,\Sigma,\mu)$ for the
underlying probability space.   Set

\Centerline{$\tilde X_n=\Bover1{b_n}X_n$}

\noindent for each $n$;  then $\sequencen{\tilde X_n}$ also is a
martingale difference sequence, and

\Centerline{$\sum_{n=1}^{\infty}\Expn(\tilde X_n^2)
=\sum_{n=1}^{\infty}\Bover1{b_n^2}\Var(X_n)<\infty$.}

\noindent By 276B, $\sequencen{\tilde X_n(\omega)}$ is summable for
almost every $\omega\in\Omega$.   But by 273Cb,

\Centerline{$\lim_{n\to\infty}\Bover{1}{b_n}\sum_{i=0}^n X_i(\omega)
=\lim_{n\to\infty}\Bover{1}{b_n}\sum_{i=0}^nb_i\tilde X_i(\omega)
=0$}

\noindent for all such $\omega$.   So we have the result.
}%end of proof of 276C

\leader{276D}{Corollary} Let $\sequencen{X_n}$ be a martingale such that
$b_n=\Expn(X_n^2)$ is finite for each $n$.

(a) If $\sup_{n\in\Bbb N}b_n$ is infinite, then
$\lim_{n\to\infty}\bover1{b_n}X_n=0$ a.e.

(b) If $\sup_{n\ge 1}\bover1nb_n<\infty$, then
$\lim_{n\to\infty}\bover1nX_n=0$ a.e.

\proof{ Consider the martingale difference sequence
$\sequencen{Y_n}=\sequencen{X_{n+1}-X_n}$.   Then
$\Expn(Y_n\times X_n)=0$, so
$\Expn(Y_n^2)+\Expn(X_{n}^2)=\Expn(X_{n+1}^2)$ for each $n$.
In particular, $\sequencen{b_n}$ must be non-decreasing.

\medskip

{\bf (a)} If $\lim_{n\to\infty}b_n=\infty$, take $m$ such that $b_m>0$;
then

\Centerline{$\sum_{n=m}^{\infty}\Bover1{b_{n+1}^2}\Var(Y_n)
=\sum_{n=m}^{\infty}\Bover1{b_{n+1}^2}(b_{n+1}-b_n)
\le\int_{b_m}^{\infty}\Bover1{t^2}dt<\infty$.}

\noindent By 276C (modifying $b_i$ for $i<m$, if necessary),

\Centerline{$\lim_{n\to\infty}\Bover1{b_n}X_n
=\lim_{n\to\infty}\Bover1{b_{n+1}}(X_0+\sum_{i=0}^nY_i)
=\lim_{n\to\infty}\Bover1{b_{n+1}}\sum_{i=0}^nY_i=0$}

\noindent almost everywhere.

\medskip

{\bf (b)} If $\gamma=\sup_{n\ge 1}\bover1nb_n<\infty$, then
$\bover1{(n+1)^2}\le\min(1,\gamma^2/t^2)$ for $b_n<t\le b_{n+1}$, so

\Centerline{$\sum_{n=0}^{\infty}\Bover1{(n+1)^2}(b_{n+1}-b_n)
\le \gamma+\gamma^2\int_{\gamma}^{\infty}\Bover1{t^2}dt<\infty$,}

\noindent and, by the same argument as before,
$\lim_{n\to\infty}\bover1nX_n=0$ a.e.

}%end of proof of 276D

\leader{276E}{`Impossibility of systems'}\cmmnt{ {\bf (a)} I return
to the word
`martingale' and the idea of a gambling system.   Consider a gambler
who takes a sequence of `fair' bets, that is, bets which have payoff
expectations of zero, but who chooses which bets to take on the basis of
past experience.   The appropriate model for such a sequence of random
events is a martingale in the sense of 275A, taking $\Sigma_n$ to be the
algebra of all events which are observable up to and including the
outcome of the $n$th bet, and $X_n$ to be the gambler's net gain at that
time.   (In this model it is natural to take
$\Sigma_0=\{\emptyset,\Omega\}$ and $X_0=\tbf{0}$.)   Certain paradoxes can
arise if we try to imagine this model with atomless $\Sigma_n$;  to
begin with it is perhaps easier to work with the discrete case, in which
each $\Sigma_n$ is finite, or is the set of unions of some countable
family of atomic events.   Now suppose that the bets involved are just
two-way bets, with two equally likely outcomes, but that the gambler
chooses his stake each time.   In this case we can think of the outcomes
as corresponding to an independent sequence $\sequencen{W_n}$ of random
variables, each taking the values $\pm 1$ with equal probability.
The gambler's system must be of the form

\Centerline{$X_{n+1}=X_n + Z_{n+1}\times W_{n+1}$,}

\noindent where $Z_{n+1}$ is his stake on the $(n+1)$-st bet, and must
be constant on each atom of the $\sigma$-algebra $\Sigma_n$ generated by
$W_1,\ldots,W_n$.   The point is that because $\int_EW_{n+1}=0$ for each
$E\in\Sigma_n$, $\Expn(Z_{n+1}\times W_{n+1})=0$, so
$\Expn(X_{n+1})=\Expn(X_n)$.

\header{276Eb}} %end of comment
{\bf (b)}\cmmnt{ The general result, of which this is a special
case, is the following.}   If $\sequencen{W_n}$ is a martingale
difference sequence adapted to $\sequencen{\Sigma_n}$, and
$\langle{Z_n}\rangle_{n\ge 1}$ is a sequence of random variables such
that (i) $Z_n$ is $\Sigma_{n-1}$-measurable (ii) $Z_n\times W_{n}$ has
finite expectation for each $n\ge 1$, then
$W_0,Z_1\times W_1,Z_2\times W_2,\ldots$
is a martingale difference sequence adapted to
$\sequencen{\Sigma_n}$\cmmnt{;  the proof that
$\int_EZ_{n+1}\times W_{n+1}=0$
for every $E\in\Sigma_n$ is exactly the argument of (b) of the proof of
276B}.

\cmmnt{
\header{276Ec}{\bf (c)} I invited you to restrict your ideas to the
discrete case for a moment;  but if you feel that you understand what it
means to say that a `system' or {\bf predictable sequence}
$\langle{Z_n}\rangle_{n\ge 1}$ must be adapted to
$\sequencen{\Sigma_n}$, in the sense that every $Z_n$ is
$\Sigma_{n-1}$-measurable, then any further difficulty lies in the
measure theory needed to show that the integrals
$\int_EZ_{n+1}\times W_{n+1}$ are zero, which is what this book is
about.

\header{276Ed}{\bf (d)} Consider the gambling system mentioned in 275Cf.
Here the idea is that $W_n=\pm 1$, as in (a), and $Z_{n+1}=2^na$ if
$X_n\le 0$, $0$ if $X_n>0$;  that is, the gambler doubles his stake each
time until he wins, and then quits.   Of course he is almost sure to win
eventually, so we have $\lim_{n\to\infty}X_n=a$ almost everywhere,
even though
$\Expn(X_n)=0$ for every $n$.   We can compute the distribution of
$X_n$:  for $n\ge 1$ we have $\Pr(X_n=a)=1-2^{-n}$,
$\Pr(X_n=-(2^n-1)a)=2^{-n}$.   Thus $\Expn(|X_n|)=(2-2^{-n+1})a$ and the
almost-everywhere convergence of the $X_n$ is an example of Doob's
Martingale Convergence Theorem.

In the language of stopping times (275N),
$X_n=\tilde Y_{\tau\wedge n}$,
where $Y_n=\sum_{k=0}^n2^kaW_k$ and $\tau=\min\{n:Y_n>0\}$.
}

\leader{*276F}{}\cmmnt{ I come now to Koml\'os' theorem.   The first
step is a trifling refinement of 276C.

\medskip

\noindent}{\bf Lemma} Let $(\Omega,\Sigma,\mu)$ be a probability space,
and $\sequencen{\Sigma_n}$ a non-decreasing sequence of
$\sigma$-subalgebras of $\Sigma$.   Suppose that $\sequencen{X_n}$ is a
sequence of random variables on $\Omega$ such that (i) $X_n$ is
$\Sigma_n$-measurable for each $n$ (ii)
$\sum_{n=0}^{\infty}\Bover1{(n+1)^2}\Expn(X_n^2)$ is finite (iii)
$\lim_{n\to\infty}X'_n=0$ a.e., where $X_n'$ is a conditional
expectation of $X_n$ on $\Sigma_{n-1}$ for each $n\ge 1$.   Then
$\lim_{n\to\infty}\Bover1{n+1}\sum_{k=0}^nX_k=0$ a.e.

\proof{ Making suitable adjustments on a negligible set if necessary, we
may suppose that $X'_n$ is $\Sigma_{n-1}$-measurable for $n\ge 1$ and
that every $X_n$ and $X'_n$ is defined on the whole of $\Omega$.   Set
$X'_0=X_0$ and $Y_n=X_n-X'_n$ for $n\in\Bbb N$.   Then $\sequencen{Y_n}$
is a martingale difference sequence adapted to $\sequencen{\Sigma_n}$.
Also $\Expn(Y_n^2)\le\Expn(X_n^2)$ for every $n$.   \Prf\ If $n\ge 1$,
$X'_n$ is square-integrable (244M), and $\Expn(Y_n\times X'_n)=0$, as in
part (b) of the proof of 276B.   Now

\Centerline{$\Expn(X_n^2)
=\Expn(Y_n+X'_n)^2
=\Expn(Y_n^2)+2\Expn(Y_n\times X'_n)+\Expn(X'_n)^2
\ge\Expn(Y_n^2)$.  \Qed}

This means that $\sum_{n=0}^{\infty}\Bover1{(n+1)^2}\Expn(Y_n^2)$ must
be finite.   By 276C,
$\lim_{n\to\infty}\Bover1{n+1}\sum_{i=0}^nY_i=0$ a.e.
But by 273Ca we also have
$\lim_{n\to\infty}\Bover1{n+1}\sum_{i=0}^nX'_i=0$ whenever
$\lim_{n\to\infty}X'_n=0$, which is almost everywhere.   So
$\lim_{n\to\infty}\Bover1{n+1}\sum_{i=0}^nX_i=0$ a.e.
}%end of proof of 276F

\leader{*276G}{Lemma} Let $(\Omega,\Sigma,\mu)$ be a probability space,
and $\sequencen{X_n}$ a sequence of random
variables on $\Omega$ such that $\sup_{n\in\Bbb N}\Expn(|X_n|)$ is
finite.   For
$k\in\Bbb N$ and $x\in\Bbb R$ set $F_k(x)=x$ if $|x|\le k$, $0$
otherwise.   Let $\Cal F$ be an ultrafilter on $\Bbb N$.

(a) For each $k\in\Bbb N$ there is a measurable function
$Y_k:\Omega\to[-k,k]$ such that
$\lim_{n\to\Cal F}\int_EF_k(X_n)=\int_EY_k$ for every $E\in\Sigma$.

(b) $\lim_{n\to\Cal F}\Expn((F_k(X_n)-Y_k)^2)
\le\lim_{n\to\Cal F}\Expn(F_k(X_n)^2)$ for each $k$.

(c) $Y=\lim_{k\to\infty}Y_k$ is defined a.e.\ and
$\lim_{k\to\infty}\Expn(|Y-Y_k|)=0$.

\proof{{\bf (a)} For each $k$, $|F_k(X_n)|\leae k\chi\Omega$ for
every $n$, so that $\{F_k(X_n):n\in\Bbb N\}$ is uniformly integrable,
and $\{F_k(X_n)^{\ssbullet}:n\in\Bbb N\}$ is relatively weakly compact
in $L^1=L^1(\mu)$ (247C).   Accordingly
$v_k=\lim_{n\to\Cal F}F_k(X_n)^{\ssbullet}$ is defined in $L^1$ for the
weak topology
(2A3Se);  take $Y_k:\Omega\to\Bbb R$ to be a measurable function
such that $Y_k^{\ssbullet}=v_k$.   For any $E\in\Sigma$,

\Centerline{$\int_EY_k
=\int v_k\times(\chi E)^{\ssbullet}
=\lim_{n\to\Cal F}\int_EF_k(X_n)$.}

\noindent In particular,

\Centerline{$|\int_EY_k|\le\sup_{n\in\Bbb N}|\int_EF_k(X_n)|\le k\mu E$}

\noindent for every $E$, so that $\{\omega:Y_k(\omega)>k\}$ and
$\{\omega:Y_k(\omega)<-k\}$ are both negligible;  changing $Y_k$ on a
negligible set if necessary, we may suppose that $|Y_k(\omega)|\le k$
for every $\omega\in\Omega$.

\medskip

{\bf (b)} Because $Y_k$ is bounded,
$Y_k^{\ssbullet}\in L^{\infty}(\mu)$, and

\Centerline{$\lim_{n\to\Cal F}\int F_k(X_n)\times Y_k
=\lim_{n\to\Cal F}\int F_k(X_n)^{\ssbullet}\times Y_k^{\ssbullet}
=\int Y_k^{\ssbullet}\times Y_k^{\ssbullet}
=\int Y_k^2$.}

\noindent Accordingly

$$\eqalignno{\lim_{n\to\Cal F}\int(F_k(X_n)-Y_k)^2
&=\lim_{n\to\Cal F}\int F_k(X_n)^2
  -2\lim_{n\to\Cal F}\int F_k(X_n)\times Y_k
  +\int Y_k^2\cr
&=\lim_{n\to\Cal F}\int F_k(X_n)^2-\int Y_k^2
\le\lim_{n\to\Cal F}\int F_k(X_n)^2.\cr}$$

\medskip

{\bf (c)} Set $W_0=Y_0=\tbf{0}$, $W_k=Y_k-Y_{k-1}$ for $k\ge 1$.   Then
$\Expn(|W_k|)\le\lim_{n\to\Cal F}\Expn(|F_k(X_n)-F_{k-1}(X_n)|)$ for
every $k\ge 1$.   \Prf\ Set $E=\{\omega:W_k(\omega)\ge 0\}$.   Then

$$\eqalign{\int_EW_k
&=\int_EY_k-\int_EY_{k-1}\cr
&=\lim_{n\to\Cal F}\int_EF_k(X_n)-\lim_{n\to\Cal F}\int_EF_{k-1}(X_n)\cr
&=\lim_{n\to\Cal F}\int_EF_k(X_n)-F_{k-1}(X_n)
\le\lim_{n\to\Cal F}\int_E|F_k(X_n)-F_{k-1}(X_n)|.\cr}$$

\noindent Similarly,

\Centerline{$|\int_{X\setminus E}W_k|
\le\lim_{n\to\Cal F}\int_{X\setminus E}|F_k(X_n)-F_{k-1}(X_n)|$.}

\noindent So

\Centerline{$\Expn(|W_k|)
=\int_EW_k-\int_{X\setminus E}W_k
\le\lim_{n\to\Cal F}\int|F_k(X_n)-F_{k-1}(X_n)|$.  \Qed}

It follows that $\sum_{k=0}^{\infty}\Expn(|W_k|)$ is finite.   \Prf\ For
any $m\ge 1$,

$$\eqalign{\sum_{k=0}^m\Expn(|W_k|)
&\le\sum_{k=1}^m\lim_{n\to\Cal F}\Expn(|F_k(X_n)-F_{k-1}(X_n)|)\cr
&=\lim_{n\to\Cal F}\Expn(\sum_{k=1}^m|F_k(X_n)-F_{k-1}(X_n)|)\cr
&=\lim_{n\to\Cal F}\Expn(|F_m(X_n)|)
\le\sup_{n\in\Bbb N}\Expn(|X_n|).\cr}$$

\noindent So
$\sum_{k=0}^{\infty}\Expn(|W_k|)\le\sup_{n\in\Bbb N}\Expn(|X_n|)$ is
finite.\ \Qed

By B.Levi's theorem (123A), $\lim_{m\to\infty}\sum_{k=0}^m|W_k|$ is
finite a.e., so that

\Centerline{$Y=\lim_{m\to\infty}Y_m=\sum_{k=0}^{\infty}W_k$}

\noindent is defined a.e.;  and moreover

\Centerline{$\Expn(|Y-Y_k|)
\le\lim_{m\to\infty}\Expn(\sum_{j=k+1}^m|W_j|)
\to 0$}

\noindent as $k\to\infty$.
}%end of proof of 276G

\leader{*276H}{Koml\'os' theorem}\cmmnt{ ({\smc Koml\'os 67})}
Let $(\Omega,\Sigma,\mu)$ be any
measure space, and $\sequencen{X_n}$ a sequence of integrable
real-valued functions on $\Omega$ such that $\sup_{n\in\Bbb N}\int|X_n|$
is finite.   Then there are a subsequence $\sequencen{X'_n}$ of
$\sequencen{X_n}$ and an integrable function $Y$ such that
$Y\eae\lim_{n\to\infty}\Bover1{n+1}\sum_{i=0}^nX''_i$
whenever $\sequencen{X''_n}$ is a subsequence of $\sequencen{X'_n}$.

\proof{ Since neither the hypothesis nor the conclusion is affected by
changing the $X_n$ on a negligible set, we may suppose throughout that
every $X_n$ is measurable and defined on the whole of $\Omega$.   In
addition, to begin with (down to the end of (e) below), let us suppose
that $\mu X=1$.   As in 276G, set $F_k(x)=x$ for $|x|\le k$, $0$ for $|x|>k$.

\medskip

{\bf (a)} Let $\Cal F$ be any non-principal ultrafilter on $\Bbb N$
(2A1O).   For $j\in\Bbb N$ set $p_j=\lim_{n\to\Cal F}\Pr(|X_n|>j)$.
Then $\sum_{j=0}^{\infty}p_j$ is finite.   \Prf\ For any $k\in\Bbb N$,

$$\eqalignno{\sum_{j=0}^kp_j
&=\sum_{j=0}^k\lim_{n\to\Cal F}\Pr(|X_n|>j)
=\lim_{n\to\Cal F}\sum_{j=0}^k\Pr(|X_n|>j)\cr
&\le\lim_{n\to\Cal F}(1+\int|X_n|)
\le 1+\sup_{n\in\Bbb N}\int|X_n|.\cr}$$

\noindent So $\sum_{j=0}^{\infty}p_j\le 1+\sup_{n\in\Bbb N}\int|X_n|$ is
finite.\ \Qed

Setting

\Centerline{$p'_j=p_j-p_{j+1}=\lim_{n\to\Cal F}\Pr(j<|X_n|\le j+1)$}

\noindent for each $j$, we have

$$\eqalign{\sum_{j=0}^{\infty}(j+1)p'_j
&=\lim_{m\to\infty}
  \bigl(\sum_{j=0}^m(j+1)p_j-\sum_{j=0}^m(j+1)p_{j+1}\bigr)\cr
&=\lim_{m\to\infty}\sum_{j=0}^mp_j-(m+1)p_{m+1}
\le\sum_{j=0}^{\infty}p_j
<\infty.\cr}$$

Next,

\Centerline{$\lim_{n\to\Cal F}\int F_k(X_n)^2
\le\sum_{j=0}^k(j+1)^2p'_j$}

\noindent for each $k$.   \Prf\ Setting
$E_{jn}=\{\omega:j<|X_n(\omega)|\le j+1\}$ for
$j$, $n\in\Bbb N$, $F_k(X_n)^2\le\sum_{j=0}^k(j+1)^2\chi E_{jn}$, so

\Centerline{$\lim_{n\to\Cal F}\int F_k(X_n)^2
\le\lim_{n\to\Cal F}\sum_{j=0}^k(j+1)^2\mu E_{jn}
=\sum_{j=0}^k(j+1)^2p'_j$.  \Qed}

\medskip

{\bf (b)} Define $\sequence{k}{Y_k}$ and $Y\eae\lim_{k\to\infty}Y_k$
from $\sequencen{X_n}$ and $\Cal F$ as in Lemma 276G.   Then

\Centerline{$J_k
=\{n:n\in\Bbb N,\,\int(F_k(X_n)-Y_k)^2\le 1+\sum_{j=0}^k(j+1)^2p'_j\}$}

\noindent belongs to $\Cal F$ for every $k\in\Bbb N$.   \Prf\ By (a)
above and 276Gb,

\Centerline{$\lim_{n\to\Cal F}\int(F_k(X_n)-Y_k)^2
\le\lim_{n\to\Cal F}\int F_k(X_n)^2
\le\sum_{j=0}^k(j+1)^2p'_j$.  \Qed}

\noindent Also, of course,

\Centerline{$K_k=\{n:n\in\Bbb N,\,\Pr(F_j(X_n)\ne X_n)\le p_j+2^{-j}$
for every $j\le k\}$}

\noindent belongs to $\Cal F$ for every $k$.

\medskip

{\bf (c)} For $n$, $k\in\Bbb N$ let $Z_{kn}$ be a simple function such
that $|Z_{kn}|\le|F_k(X_n)-Y_k|$ and
$\int|F_k(X_n)-Y_k-Z_{kn}|\le 2^{-k}$.   For $m\in\Bbb N$ let $\Sigma_m$
be the algebra of subsets of $\Omega$ generated by sets of the form
$\{\omega:Z_{kn}(\omega)=\alpha\}$ for $k$, $n\le m$ and
$\alpha\in\Bbb R$.   Because each $Z_{kn}$ takes only finitely many
values, $\Sigma_m$ is finite (and is therefore a $\sigma$-subalgebra of
$\Sigma$);  and of course $\Sigma_m\subseteq\Sigma_{m+1}$ for every $m$.

We need to look at conditional expectations on the $\Sigma_m$, and
because $\Sigma_m$ is always finite these have a particularly
straightforward
expression.   Let $\Cal A_m$ be the set of `atoms', or minimal non-empty
sets, in $\Sigma_m$;  that is, the set of equivalence classes in
$\Omega$ under the relation $\omega\sim\omega'$ if
$Z_{kn}(\omega)=Z_{kn}(\omega')$ for all $k$, $n\le m$.   For any
integrable random variable $X$ on $\Omega$, define $\Expn_m(X)$ by
setting

$$\eqalign{\Expn_m(X)(\omega)
&=\Bover1{\mu A}\int_AX\text{ if }x\in A\in\Cal A_m
  \text{ and }\mu A>0,\cr
&=0\text{ if }x\in A\in\Cal A_m\text{ and }\mu A=0.\cr}$$

\noindent Then $\Expn_m(X)$ is a conditional expectation of $X$ on
$\Sigma_m$.
%It will be useful later to note that $\Expn_l(\Expn_m(X))=\Expn_l(X)$
%whenever $l\le m$.

Now

$$\eqalignno{\lim_{n\to\Cal F}\int|\Expn_m(F_k(X_n)-Y_k)|
&=\lim_{n\to\Cal F}\sum_{A\in\Cal A_m}\int_A|\Expn_m(F_k(X_n)-Y_k)|\cr
&=\lim_{n\to\Cal F}\sum_{A\in\Cal A_m}|\int_A\Expn_m(F_k(X_n)-Y_k)|\cr
\displaycause{because $\Expn_m(F_k(X_n)-Y_k)$ is constant on each
$A\in\Cal A_m$}
&=\lim_{n\to\Cal F}\sum_{A\in\Cal A_m}|\int_AF_k(X_n)-Y_k|\cr
&=\sum_{A\in\Cal A_m}\lim_{n\to\Cal F}|\int_AF_k(X_n)-Y_k|
=0\cr}$$

\noindent by the choice of $Y_k$.   So if we set

\Centerline{$I_m
=\{n:n\in\Bbb N,\,\int|\Expn_m(F_k(X_n)-Y_k)|\le 2^{-k}$ for every
$k\le m\}$,}

\noindent then $I_m\in\Cal F$ for every $m$.

\medskip

{\bf (d)} Suppose that $\sequencen{r(n)}$ is any strictly increasing
sequence in $\Bbb N$ such that $r(0)>0$, $r(n)\in J_n\cap K_n$ for every
$n$ and
$r(n)\in I_{r(n-1)}$ for $n\ge 1$.   Then
$\Bover1{n+1}\sum_{i=0}^nX_{r(i)}\to Y$ a.e.\ as $n\to\infty$.
\Prf\   Express $X_{r(n)}$ as

\Centerline{$(X_{r(n)}-F_n(X_{r(n)}))
+(F_n(X_{r(n)})-Y_n-Z_{n,r(n)})+Y_n+Z_{n,r(n)}$}

\noindent for each $n$.   Taking these pieces in turn:

\medskip

\quad{\bf (i)}

$$\eqalignno{\sum_{n=0}^{\infty}\Pr(X_{r(n)}\ne F_n(X_{r(n)}))
&\le\sum_{n=0}^{\infty}p_n+2^{-n}\cr
\displaycause{because $r(n)\in K_n$ for every $n$}
&<\infty\cr}$$

\noindent by (a).   But this means that
$X_{r(n)}-F_n(X_{r(n)})\to 0$ a.e.,
since the sequence is eventually zero at almost every point, and
$\Bover1{n+1}\sum_{i=0}^nX_{r(i)}-F_i(X_{r(i)})\to 0$ a.e.\ by 273Ca again.

\medskip

\quad{\bf (ii)} By the choice of the $Z_{n,r(n)}$,

\Centerline{$\sum_{n=0}^{\infty}\int|F_n(X_{r(n)})-Y_n-Z_{n,r(n)}|
\le\sum_{n=0}^{\infty}2^{-n}$}

\noindent is finite, so $F_n(X_{r(n)})-Y_n-Z_{n,r(n)}\to 0$ a.e.\ and
$\Bover1{n+1}\sum_{i=0}^nF_i(X_{r(i)})-Y_i-Z_{i,r(i)}\to 0$ a.e.

\medskip

\quad{\bf (iii)} By 276G, $Y_n\to Y$ a.e.\ and
$\Bover1{n+1}\sum_{i=0}^nY_i\to Y$ a.e.

\medskip

\quad{\bf (iv)} We know that, for each $n\ge 1$,
$r(n)\in I_{r(n-1)}$.   So (because $r(n-1)\ge n$)
$\int|\Expn_{r(n-1)}(F_n(X_{r(n)})-Y_n)|\le 2^{-n}$.   But as also

\Centerline{$\int\bigl|\Expn_{r(n-1)}
  \bigl(F_n(X_{r(n)})-Y_n-Z_{n,r(n)}\bigr)\bigr|
\le\int|F_n(X_{r(n)})-Y_n-Z_{n,r(n)}|\le 2^{-n}$}

\noindent by 244M and the choice of $Z_{n,r(n)}$,

$$\eqalign{\int|\Expn_{r(n-1)}Z_{n,r(n)}|
&=\int\bigl|\Expn_{r(n-1)}(F_n(X_{r(n)})-Y_n)
  -\Expn_{r(n-1)}(F_n(X_{r(n)})-Y_n-Z_{n,r(n)})\bigr|\cr
&\le 2^{-n+1}\cr}$$

\noindent for every $n$.   Accordingly $\Expn_{r(n-1)}Z_{n,r(n)}\to 0$
a.e.

On the other hand,

$$\eqalignno{\sum_{n=0}^{\infty}\Bover1{(n+1)^2}\int Z_{n,r(n)}^2
&\le\sum_{n=0}^{\infty}\Bover1{(n+1)^2}\int F_n(X_{r(n)}-Y_n)^2\cr
&\le\sum_{n=0}^{\infty}\Bover1{(n+1)^2}(1+\sum_{j=0}^n(j+1)^2p'_j)\cr
\displaycause{because $r(n)\in J_n$}
&\le\sum_{n=0}^{\infty}\Bover1{(n+1)^2}
  +\sum_{j=0}^{\infty}(j+1)^2p'_j\sum_{n=j}^{\infty}\Bover1{(n+1)^2}\cr
&\le\Bover{\pi^2}6
  +2\sum_{j=0}^{\infty}(j+1)p'_j\cr}$$

\noindent is finite.   (I am using the estimate

\Centerline{$\sum_{n=j}^{\infty}\Bover1{(n+1)^2}
\le\sum_{n=j}^{\infty}\Bover2{n+1}-\Bover2{n+2}=\Bover2{j+1}$.)}

\noindent By 276F, applied to $\sequencen{\Sigma_{r(n)}}$
and $\sequencen{Z_{n,r(n)}}$, $\Bover1{n+1}\sum_{i=0}^nZ_{i,r(i)}\to 0$
a.e.

\medskip

\quad{\bf (v)} Adding these four components, we see that
$\Bover1{n+1}\sum_{i=0}^nX_{r(i)}\to 0$, as claimed.\ \Qed

\medskip

{\bf (e)} Now fix any strictly increasing sequence $\sequencen{s(n)}$ in
$\Bbb N$ such that $s(0)>0$, $s(n)\in\bigcap_{m\le n}J_m\cap K_n$
for every $n$ and
$s(n)\in I_{s(n-1)}$ for $n\ge 1$;  such a sequence exists because
$J_m\cap K_m\cap I_{s(n-1)}$ belongs to $\Cal F$, so is infinite, for
every $n\ge 1$ and $m\in\Bbb N$.   Set $X'_n=X_{s(n)}$ for every $n$.   If
$\sequencen{X''_n}$ is a subsequence of $\sequencen{X'_n}$, then it is
of the form $\sequencen{X_{s(r(n))}}$ for some strictly increasing
sequence $\sequencen{r(n)}$.   In this case

\Centerline{$s(r(0))\ge s(0)>0$,}

\Centerline{$s(r(n))\in J_{r(n)}\cap K_{r(n)}\subseteq J_n\cap K_n$ for
every $n$,}

\Centerline{$s(r(n))\in I_{s(r(n)-1)}\subseteq I_{s(r(n-1))}$ for every
$n\ge 1$.}

\noindent So (d) tells us that $\Bover1{n+1}\sum_{i=0}^nX''_i\to Y$ a.e.

\medskip

{\bf (f)} Thus the theorem is proved in the case in which
$(\Omega,\Sigma,\mu)$ is a probability space.   Now suppose that $\mu$
is $\sigma$-finite and $\mu\Omega>0$.   In this case there is a strictly
positive measurable function $f:\Omega\to\Bbb R$ such that
$\int fd\mu=1$ (215B(ix)).   Let $\nu$ be the corresponding
indefinite-integral measure (234J), so that $\nu$ is a probability
measure on $\Omega$, and $\sequencen{\Bover1f\times X_n}$ is a sequence
of $\nu$-integrable functions such that
$\sup_{n\in\Bbb N}\int\Bover1f\times X_nd\nu$ is finite (235K).   From
(a)-(e) we see that there must be a $\nu$-integrable function $Y$ and a
subsequence $\sequencen{X'_n}$ of $\sequencen{X_n}$ such that
$\Bover1{n+1}\sum_{i=0}^n\Bover1f\times X''_i\to Y\,\,\nu$-a.e.\ for
every subsequence $\sequencen{X''_n}$ of $\sequencen{X'_n}$.   But $\mu$
and $\nu$ have the same negligible sets (234Lc), so
$\Bover1{n+1}\sum_{i=0}^nX''_i\to f\times Y\,\,\mu$-a.e.\ for every
subsequence $\sequencen{X''_n}$ of $\sequencen{X'_n}$.

\medskip

{\bf (g)} Since the result is trivial if $\mu\Omega=0$, the theorem is
true whenever $\mu$ is $\sigma$-finite.   For the general case, set

\Centerline{$\tilde\Omega
=\bigcup_{n\in\Bbb N}\{\omega:X_n(\omega)\ne 0\}
=\bigcup_{m,n\in\Bbb N}\{\omega:|X_n(\omega)|\ge 2^{-m}\}$,}

\noindent so that the subspace measure $\mu_{\tilde\Omega}$ is
$\sigma$-finite.   Then there are a $\mu_{\tilde\Omega}$-integrable
function $\tilde Y$ and a subsequence $\sequencen{X'_n}$ of
$\sequencen{X_n}$ such that
$\Bover1{n+1}\sum_{i=0}^nX''_i\restrp\tilde\Omega\to\tilde Y\,\,
\mu_{\tilde\Omega}$-a.e.\ for every subsequence $\sequencen{X''_n}$ of
$\sequencen{X'_n}$.   Setting $Y(\omega)=\tilde Y(\omega)$ if
$\omega\in\tilde\Omega$, $0$ for $\omega\in\Omega\setminus\tilde\Omega$,
we see that $Y$ is $\mu$-integrable and that
$\Bover1{n+1}\sum_{i=0}^nX''_i\to Y\,\,\mu$-a.e. whenever
$\sequencen{X''_n}$ is a subsequence of $\sequencen{X'_n}$.   This
completes the proof.
}%end of proof of 276H

\exercises{\leader{276X}{Basic exercises $\pmb{>}$(a)}
%\sqheader 276Xa
Let $\sequencen{X_n}$ be a martingale adapted to
a sequence $\sequencen{\Sigma_n}$ of $\sigma$-algebras.   Show that
$\int_EX_n^2\le\int_EX_{n+1}^2$ whenever $n\in\Bbb N$ and $E\in\Sigma_n$
(allowing $\infty$ as a value of an integral).   \Hint{see the proof of
276B.}
%276B

\sqheader 276Xb Let $\sequencen{X_n}$ be a martingale.   Show
that for any $\epsilon>0$,

\Centerline{$\Pr(\sup_{n\in\Bbb N}|X_n|\ge\epsilon)
\le\Bover1{\epsilon^2}\sup_{n\in\Bbb N}\Expn(X_n^2)$.}

\noindent\Hint{put 276Xa together with the argument for 275D.}
%276Xa, 276B

\spheader 276Xc When does 276Xb give a sharper result than 275Xb?
%276Xb,  276B

\spheader 276Xd Let $\sequencen{X_n}$ be an independent identically
distributed sequence of random variables with zero expectation and non-zero
finite variance, and $\sequencen{t_n}$ a sequence in $\Bbb R$.   Show that
(i) if $\sum_{n=0}^{\infty}t_n^2<\infty$, then $\sum_{n=0}^{\infty}t_nX_n$
is defined in $\Bbb R$ a.e.  (ii) if $\sum_{n=0}^{\infty}t_n^2=\infty$ then
$\sum_{n=0}^{\infty}t_nX_n$ is undefined a.e.   \Hint{276B, 274Xj.}
%276B

\spheader 276Xe Let $\sequencen{X_n}$ be a martingale difference
sequence and set $Y_n=\bover1{n+1}(X_0+\ldots+X_n)$ for each
$n\in\Bbb N$.   Show that if $\sequencen{X_n}$ is uniformly integrable
then $\lim_{n\to\infty}\|Y_n\|_1=0$.   \Hint{use the argument of 273Na,
with 276C in place of 273D, and setting $\tilde X_n=X'_n-Z_n$, where
$Z_n$ is an appropriate conditional expectation of $X'_n$.}
%276C

\spheader 276Xf Suppose that
$\sequencen{X_n}$ is a uniformly bounded martingale
difference sequence and $\sequencen{a_n}\in\ell^2$.
Show that $\lim_{n\to\infty}\prod_{i=0}^n(1+a_iX_i)$ is defined and finite
almost everywhere.   \Hint{$\sequencen{a_nX_n}$ is summable and
square-summable a.e.}
%276C

\sqheader 276Xg\dvAformerly{2{}76Xe}{\bf Strong law of large numbers:  
fifth form} A sequence $\sequencen{X_n}$ of random variables is
{\bf exchangeable} if $(X_{n_0},\ldots,X_{n_k})$ has the same
joint distribution as $(X_0,\ldots,X_k)$ whenever $n_0,\ldots,n_k$ are
distinct.
Show that if $\sequencen{X_n}$ is an exchangeable sequence of random
variables with finite expectation, then
$\sequencen{\Bover1{n+1}\sum_{i=0}^{\infty}X_i}$ converges a.e.
\Hint{276H.}
%276H

\leader{276Y}{Further exercises (a)}
%\spheader 276Ya
Let $\sequencen{X_n}$ be a martingale difference
sequence such that $\sup_{n\in\Bbb N}
\ifdim\pagewidth>467pt\penalty-100\fi
\|X_n\|_p$ is finite, where
$p\in\ooint{1,\infty}$.   Show that
$\lim_{n\to\infty}\|\bover1{n+1}\sum_{i=0}^nX_i\|_p=0$.   \Hint{273Nb.}
%276A

\spheader 276Yb Let $\sequencen{X_n}$ be a uniformly integrable
martingale difference sequence and $Y$ a bounded random variable.   Show
that $\lim_{n\to\infty}\Expn(X_n\times Y)=0$.   (Compare 272Ye.)
%276A

\spheader 276Yc Use 275Yh to prove 276Xa.
%276Xa, 276B

\spheader 276Yd Let $\sequencen{X_n}$ be a sequence of random
variables such that, for some $\delta>0$,
$\sup_{n\in\Bbb N}n^{\delta}\Expn(|X_n|)$ is finite.   Set
$S_n=\bover1{n+1}(X_0+\ldots+X_n)$ for each $n$.   Show that
$\lim_{n\to\infty}S_n=0$ a.e.   \Hint{set
$Z_k=2^{-k}(|X_0|+\ldots+|X_{2^k-1}|)$.    Show that
$\sum_{k=0}^{\infty}\Expn(Z_k)<\infty$.   Show that $|S_n|\le 2Z_{k+1}$
if $2^k<n\le 2^{k+1}$.}
%for 276Ye

\spheader 276Ye {\bf Strong law of large numbers:  sixth form} Let
$\sequencen{X_n}$ be a martingale difference sequence such that, for
some $\delta>0$, $\sup_{n\in\Bbb N}\Expn(|X_n|^{1+\delta})$ is finite.
Set $S_n=\bover1{n+1}(X_0+\ldots+X_n)$ for each $n$.   Show that
$\lim_{n\to\infty}S_n=0$ a.e.   \Hint{take a non-decreasing
sequence $\sequencen{\Sigma_n}$ to which $\sequencen{X_n}$ is adapted.
Set $Y_n=X_n$ when $|X_n|\le n$, $0$ otherwise.   Let $U_n$ be a
conditional expectation of $Y_n$ on $\Sigma_{n-1}$ and set
$Z_n=Y_n-U_n$.   Use ideas from 273H, 276C and 276Yd above to show that
$\bover{1}{n+1}\sum_{i=0}^nV_i\to 0$ a.e.\ for $V_i=Z_i$, $V_i=U_i$,
$V_i=X_i-Y_i$.}
%276Yd, 276C mt27bits

\spheader 276Yf Show that there is a martingale $\sequencen{X_n}$ which
converges in measure but is not convergent a.e.   (Compare 273Ba.)
\Hint{arrange that
$\{\omega:X_{n+1}(\omega)\ne 0\}=E_n
\subseteq\{\omega:|X_{n+1}(\omega)-X_n(\omega)|\ge 1\}$, where
$\sequencen{E_n}$ is an independent sequence of sets and
$\mu E_n=\Bover1{n+1}$ for each $n$.}
%with 276Yg

\spheader 276Yg Give an example of an identically distributed
martingale difference sequence $\sequencen{X_n}$ such that
$\sequencen{\bover1{n+1}(X_0+\ldots+X_n)}$ does not
converge to $0$ almost everywhere.   \Hint{start by devising a
uniformly bounded sequence $\sequencen{U_n}$ such that
$\lim_{n\to\infty}\Expn(|U_n|)=0$ but
$\sequencen{\bover1{n+1}(U_0+\ldots+U_n)}$ does not converge to $0$
almost everywhere.   Now repeat your construction in such a context that
the $U_n$ can be derived from an identically distributed martingale
difference sequence by the formulae of 276Ye.}
%276Ye, 276C

\spheader 276Yh Construct a proof of Koml\'os' theorem which does not
involve ultrafilters, or any other use of the full axiom of choice, but
proceeds throughout by selecting appropriate sub-subsequences.
Remember to check that you can prove any fact you use about weakly
convergent sequences in $L^1$ on the same rules.
}%end of exercises

\endnotes{
\Notesheader{276}
I include two more versions of the strong law of large numbers (276C,
276Ye) not because I have any applications in mind but because I think
that if you know the strong law for $\|\,\|_{1+\delta}$-bounded
independent sequences, and what a martingale difference
sequence is, then there is something missing if you do not know the
strong law for $\|\,\|_{1+\delta}$-bounded martingale difference
sequences.   And then, of course, I have to add 276Yf and 276Yg (which
seems to be difficult), lest
you be tempted to think that the strong law is `really' about martingale
difference sequences rather than about independent sequences.
(Compare 272Yd and 275Xl.)

Koml\'os' theorem is rather outside the scope of this volume;  it is
quite hard work and surely much less important, to most probabilists,
than many results I have omitted.   It does provide a quick proof of
276Xg.   However it is relevant to questions arising in some topics
treated in Volumes 3 and 4, and the proof fits naturally into this
section.
}%end of notes

\discrpage

