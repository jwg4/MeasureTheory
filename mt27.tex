\frfilename{mt27.tex} 
\versiondate{26.8.13} 
\copyrightdate{1996} 
 
\loadeusm 
\def\hatcalBnu{\hat{\Cal B}_{\nu}} 
 
\def\chaptername{Probability theory} 
\def\sectionname{Introduction} 
 
\newchapter{27} 
 
Lebesgue created his theory of integration in response to a 
number of problems in real analysis, and all his life seems to have 
thought of it as a tool for use in geometry and calculus  
({\smc Lebesgue 72}, vols.\ 1 and 2).   Remarkably, it 
turned out, when suitably adapted, to provide a solid foundation for 
probability theory.   The development of this approach is generally 
associated with the name of Kolmogorov.   It has so come to 
dominate modern abstract probability theory that many authors ignore all 
other methods.   I do not propose to commit myself to any view on 
whether 
$\sigma$-additive measures are the only way to give a rigorous 
foundation to probability theory, or whether they are adequate to deal 
with all probabilistic ideas;  there are some serious philosophical 
questions here, since probability theory, at least in its applied 
aspects, seeks to help us to understand the material world outside mathematics. 
But from my position as a measure theorist, it is 
incontrovertible that probability theory is among the central 
applications of the concepts and theorems of measure theory, and is one 
of the most vital sources of new ideas;  and that every measure theorist 
must be alert to the intuitions which probabilistic methods can provide. 
 
I have written the preceding paragraph in terms suggesting that  
`probability theory' is somehow distinguishable from the rest of measure 
theory;  this is another point on which I should prefer not to put 
forward any opinion as definitive.   But undoubtedly there is a 
distinction, rather deeper than the elementary point that probability 
deals (almost) exclusively with spaces of measure $1$. 
M.Lo\`eve argues persuasively ({\smc Lo\`eve 77}, $\S$10.2) 
that the essence of 
probability theory is the artificial nature of the probability spaces 
themselves.   In measure theory, when we wish to integrate a function, 
we usually feel that we have a proper function with a domain and 
values.   In probability theory, when we take the expectation of a 
random variable, the variable is an `observable' or `the result 
of an experiment';  we are generally uncertain, or ignorant, or 
indifferent concerning the factors underlying the variable.   Let me 
give an example from the theorems below.   In the proof of the Central 
Limit Theorem (274F), I find that I need an auxiliary list 
$Z_0,\ldots,Z_n$ of random variables, independent of each other and of 
the original sequence $X_0,\ldots,X_n$.   I create such a sequence by 
taking a product space $\Omega\times\Omega'$, and writing 
$X'_i(\omega,\omega')=X_i(\omega)$, while the $Z_i$ are functions of 
$\omega'$.   Now the difference between the $X_i$ and the $X'_i$ is of a 
type which a well-trained analyst would ordinarily take seriously.   We 
do not think that the function $x\mapsto x^2:[0,1]\to[0,1]$ is the 
same thing as the function $(x_1,x_2)\mapsto x_1^2:[0,1]^2\to[0,1]$.   But a 
probabilist is likely to feel that it is positively pedantic to start 
writing $X'_i$ instead of $X_i$.   He did not believe in the space 
$\Omega$ in the first place, and if it turns out to be inadequate for 
his intuition he enlarges it without a qualm.   Lo\`eve calls 
probability 
spaces `fictions', `inventions of the imagination' in Larousse's 
words;  they are necessary in the models Kolmogorov has taught us 
to use, but we have a vast amount of freedom in choosing them, and in 
their essence they are nothing so definite as a set with points. 
 
A probability space, therefore, is somehow a more shadowy entity in 
probability theory than it is in measure theory.   The important objects 
in probability theory are random variables and distributions, 
particularly joint distributions.   In this volume I shall deal 
exclusively with random variables which can be thought of as 
taking values in some power of $\Bbb R$;  but this is not the central 
point.   What is vital is that somehow the {\it codomain}, the potential 
set of values, of a random variable, is much better defined than its 
{\it domain}.   Consequently our attention is focused not on any 
features of the artificial space which it is convenient to use as the 
underlying probability space -- I write `underlying', though 
it is the most superficial and easily changed aspect of the model 
-- but on the distribution on the codomain induced by the random 
variable.   Thus the Central Limit Theorem, which speaks only of 
distributions, is actually more important in applied probability than 
the Strong Law of Large Numbers, which claims to tell us what a 
long-term average will almost certainly be. 
 
W.Feller ({\smc Feller 66}) goes even farther than Lo\`eve, and as 
far as 
possible works entirely with distributions, setting up machinery which 
enables him to go for long stretches without mentioning probability 
spaces at all.   I make no attempt to emulate him.   But the approach is 
instructive and faithful to the essence of the subject. 
 
Probability theory includes more mathematics than can easily be 
encompassed in a lifetime, and I have selected for this introductory 
chapter the two limit theorems I have already mentioned, the Strong Law 
of Large Numbers and the Central Limit Theorem,  
together with some material on martingales (\S\S275-276).    
They illustrate not 
only the special character of probability theory -- so that you will be 
able to form your own judgement on the remarks above -- but also some of  
its chief contributions to `pure' measure theory, the concepts of  
`independence' and `conditional expectation'. 
 
 
\discrpage 
 
